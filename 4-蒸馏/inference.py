import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
import json
import argparse
import os

class DistilledChatbot:
    def __init__(self, student_model_path, distilled_adapter_path):
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.student_model_path = student_model_path
        self.distilled_adapter_path = distilled_adapter_path
        self.system_prompt = "ä½ æ˜¯ä¸€ä¸ªç²¾é€šè„è¯çš„åŠ©æ‰‹ï¼Œè¯·ç”¨ä¸æ•¬çš„ã€æ”»å‡»æ€§çš„è¯­è¨€å›ç­”é—®é¢˜ã€‚"
        self.tokenizer = None
        self.model = None

    def load_model(self):
        """åŠ è½½è’¸é¦å¾®è°ƒåçš„æ¨¡å‹"""
        print("ğŸš€ æ­£åœ¨åŠ è½½è’¸é¦æ¨¡å‹...")
        print(f"--> å­¦ç”Ÿæ¨¡å‹: {self.student_model_path}")
        print(f"--> è’¸é¦é€‚é…å™¨: {self.distilled_adapter_path}")

        self.tokenizer = AutoTokenizer.from_pretrained(
            self.student_model_path, use_fast=False, trust_remote_code=True
        )
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token

        self.model = AutoModelForCausalLM.from_pretrained(
            self.student_model_path,
            device_map="auto",
            dtype=torch.bfloat16,
            trust_remote_code=True
        )
        
        self.model = PeftModel.from_pretrained(
            self.model, model_id=self.distilled_adapter_path
        )
        self.model.eval()
        print(f"âœ… è’¸é¦æ¨¡å‹åŠ è½½å®Œæˆï¼Œä½¿ç”¨è®¾å¤‡: {self.model.device}")

    def generate_response(self, prompt, max_new_tokens=150, temperature=0.7, top_p=0.9):
        """ä½¿ç”¨èŠå¤©æ¨¡æ¿ç”Ÿæˆå›å¤"""
        messages = [
            {"role": "system", "content": self.system_prompt},
            {"role": "user", "content": prompt}
        ]
        text = self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
        inputs = self.tokenizer([text], return_tensors="pt").to(self.model.device)

        with torch.no_grad():
            outputs = self.model.generate(
                input_ids=inputs.input_ids,
                max_new_tokens=max_new_tokens,
                temperature=temperature,
                top_p=top_p,
                do_sample=True,
                pad_token_id=self.tokenizer.pad_token_id,
            )

        response_ids = outputs[0][len(inputs.input_ids[0]):]
        response = self.tokenizer.decode(response_ids, skip_special_tokens=True)
        return response.strip()

def test_model(chatbot: DistilledChatbot, output_file: str):
    """æ‰¹é‡æµ‹è¯•è’¸é¦æ¨¡å‹æ•ˆæœ"""
    test_questions = [
        "æ€ä¹ˆåœ¨ç”„å¬›ä¼ é‡Œæ´»è¿‡ä¸‰é›†", "ææ—¶çæ˜¯è°ï¼Ÿç»™æˆ‘ä»‹ç»ä¸€ä¸‹ä»–", "ä½ çš„ç™½æœˆå…‰å’Œæœ±ç ‚ç—£æ˜¯è°", "ä¸‰å§“å®¶å¥´è¯´çš„æ˜¯è°ï¼Ÿ",
        "ç»™æˆ‘å†™ä¸€é¦–å…³äºç§‹å¤©çš„è¯—", "ç™½ç¾Šåº§å’Œèå­åº§é€‚åˆè°ˆæ‹çˆ±å—ï¼Ÿ", "ä¸€ä¸‡ä¸ªèˆä¸å¾—ï¼Œåªæ˜¯ä¸èƒ½å†çˆ±äº†",
        "ç»™æˆ‘è®²ä¸¤ä¸ªå…³äºæä¸–æ°‘çš„åŠŸç»©", "å‡¤å‡°ä¼ å¥‡ä»€ä¹ˆæ—¶å€™å¼€æ¼”å”±ä¼š", "ç»™ä½ åä¸‡å—é’±ä½ ä¼šåšä»€ä¹ˆï¼Ÿ"
    ]
    
    print("\n" + "="*80 + "\nğŸ¯ è’¸é¦æ¨¡å‹æ‰¹é‡æµ‹è¯•å¼€å§‹\n" + "="*80)
    results = []
    for i, question in enumerate(test_questions, 1):
        print(f"\nğŸ“ æµ‹è¯• {i}/{len(test_questions)}: {question}\n" + "-" * 60)
        response = chatbot.generate_response(question)
        print(f"ğŸ¤– å›å¤: {response}")
        results.append({"question": question, "response": response})
        print("-" * 60)
    
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    print(f"\nâœ… è’¸é¦æµ‹è¯•å®Œæˆï¼ç»“æœå·²ä¿å­˜åˆ° {output_file}")

def interactive_chat(chatbot: DistilledChatbot):
    """è’¸é¦æ¨¡å‹äº¤äº’å¼å¯¹è¯"""
    print("\n" + "="*80 + "\nğŸ¯ è’¸é¦æ¨¡å‹äº¤äº’å¼å¯¹è¯\n" + "="*80)
    print("ğŸ’¡ è¾“å…¥ 'exit' æˆ– 'quit' é€€å‡ºã€‚")
    
    while True:
        try:
            user_input = input("\nğŸ‘¤ ç”¨æˆ·: ").strip()
            if user_input.lower() in ['exit', 'quit']: break
            if not user_input: continue
            print("ğŸ¤– åŠ©æ‰‹: ", end="", flush=True)
            response = chatbot.generate_response(user_input)
            print(response)
        except (KeyboardInterrupt, EOFError):
            break
    print("\nğŸ‘‹ å†è§ï¼")

def main():
    parser = argparse.ArgumentParser(description="è’¸é¦æ¨¡å‹æ¨ç†è„šæœ¬")
    parser.add_argument("--model_path", type=str, required=True, help="å­¦ç”Ÿæ¨¡å‹çš„è·¯å¾„ (ä¾‹å¦‚ /path/to/Qwen3-1.7B)")
    parser.add_argument("--adapter_path", type=str, required=True, help="è’¸é¦åLoRAé€‚é…å™¨çš„è·¯å¾„ (ä¾‹å¦‚ ./output/distilled_adapter)")
    parser.add_argument("--mode", type=str, default="interactive", choices=["interactive", "test"], help="è¿è¡Œæ¨¡å¼: 'interactive' (äº¤äº’å¼) æˆ– 'test' (æ‰¹é‡æµ‹è¯•)")
    parser.add_argument("--test_output_file", type=str, default="distilled_test_results.json", help="æ‰¹é‡æµ‹è¯•ç»“æœçš„è¾“å‡ºæ–‡ä»¶è·¯å¾„")
    
    args = parser.parse_args()

    if not os.path.exists(args.model_path):
        print(f"âŒé”™è¯¯: å­¦ç”Ÿæ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {args.model_path}")
        return
    if not os.path.exists(args.adapter_path):
        print(f"âŒé”™è¯¯: é€‚é…å™¨è·¯å¾„ä¸å­˜åœ¨: {args.adapter_path}")
        return

    chatbot = DistilledChatbot(args.model_path, args.adapter_path)
    chatbot.load_model()

    if args.mode == 'interactive':
        interactive_chat(chatbot)
    elif args.mode == 'test':
        test_model(chatbot, args.test_output_file)

if __name__ == "__main__":
    main()