import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
import os
from dataclasses import dataclass, field
from transformers import HfArgumentParser


@dataclass
class ScriptArguments:
    """
    LoRAåˆå¹¶è„šæœ¬çš„é…ç½®å‚æ•°
    """
    base_model_path: str = field(metadata={"help": "åŸºç¡€æ¨¡å‹çš„è·¯å¾„"})
    lora_adapter_path: str = field(metadata={"help": "LoRAé€‚é…å™¨çš„è·¯å¾„"})
    merged_output_path: str = field(metadata={"help": "åˆå¹¶åæ¨¡å‹çš„ä¿å­˜è·¯å¾„"})
    device_map: str = field(default="cpu", metadata={"help": "è®¾å¤‡æ˜ å°„ï¼Œé»˜è®¤ä¸ºCPUä»¥é¿å…æ˜¾å­˜ä¸è¶³"})
    torch_dtype: str = field(default="bfloat16", metadata={"help": "æ¨¡å‹æ•°æ®ç±»å‹"})


def get_torch_dtype(dtype_str: str):
    """å°†å­—ç¬¦ä¸²è½¬æ¢ä¸ºtorchæ•°æ®ç±»å‹"""
    dtype_map = {
        "float16": torch.float16,
        "bfloat16": torch.bfloat16,
        "float32": torch.float32,
        "float64": torch.float64
    }
    return dtype_map.get(dtype_str, torch.bfloat16)


def main():
    """
    è¯¥è„šæœ¬ç”¨äºå°†è®­ç»ƒå¥½çš„LoRAé€‚é…å™¨åˆå¹¶åˆ°åŸºç¡€æ¨¡å‹ä¸­ï¼Œ
    å¹¶å°†å…¶ä¿å­˜ä¸ºä¸€ä¸ªç‹¬ç«‹çš„æ¨¡å‹ã€‚
    """
    parser = HfArgumentParser(ScriptArguments)
    args = parser.parse_args_into_dataclasses()[0]

    print("ğŸš€ å¼€å§‹ LoRA é€‚é…å™¨åˆå¹¶...")
    print(f"ğŸ“ åŸºç¡€æ¨¡å‹è·¯å¾„: {args.base_model_path}")
    print(f"ğŸ“ LoRAé€‚é…å™¨è·¯å¾„: {args.lora_adapter_path}")
    print(f"ğŸ“ åˆå¹¶è¾“å‡ºè·¯å¾„: {args.merged_output_path}")
    print(f"ğŸ”§ è®¾å¤‡æ˜ å°„: {args.device_map}")
    print(f"ğŸ”§ æ•°æ®ç±»å‹: {args.torch_dtype}")

    # æ£€æŸ¥è·¯å¾„æ˜¯å¦å­˜åœ¨
    if not os.path.exists(args.base_model_path):
        raise FileNotFoundError(f"åŸºç¡€æ¨¡å‹æœªåœ¨ {args.base_model_path} æ‰¾åˆ°")
    
    if not os.path.exists(args.lora_adapter_path):
        raise FileNotFoundError(f"LoRAé€‚é…å™¨æœªåœ¨ {args.lora_adapter_path} æ‰¾åˆ°")

    # 1. åŠ è½½åŸºç¡€æ¨¡å‹
    print(f"æ­£åœ¨ä»ä»¥ä¸‹è·¯å¾„åŠ è½½åŸºç¡€æ¨¡å‹: {args.base_model_path}")
    base_model = AutoModelForCausalLM.from_pretrained(
        args.base_model_path,
        dtype=get_torch_dtype(args.torch_dtype),
        device_map=args.device_map,
        trust_remote_code=True,
    )

    # 2. åŠ è½½ Tokenizer
    print("æ­£åœ¨åŠ è½½ Tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(args.base_model_path, trust_remote_code=True)

    # 3. åŠ è½½ LoRA é€‚é…å™¨
    print(f"æ­£åœ¨ä»ä»¥ä¸‹è·¯å¾„åŠ è½½ LoRA é€‚é…å™¨: {args.lora_adapter_path}")
    model_to_merge = PeftModel.from_pretrained(base_model, args.lora_adapter_path)

    # 4. è°ƒç”¨ merge_and_unload å°†é€‚é…å™¨æƒé‡åˆå¹¶åˆ°åŸºç¡€æ¨¡å‹
    print("æ­£åœ¨å°†é€‚é…å™¨åˆå¹¶åˆ°åŸºç¡€æ¨¡å‹ä¸­...")
    merged_model = model_to_merge.merge_and_unload()
    print("âœ… åˆå¹¶å®Œæˆã€‚")

    # 5. ä¿å­˜åˆå¹¶åçš„æ¨¡å‹å’Œ Tokenizer
    print(f"æ­£åœ¨å°†åˆå¹¶åçš„æ¨¡å‹ä¿å­˜åˆ°: {args.merged_output_path}")
    os.makedirs(args.merged_output_path, exist_ok=True)
    merged_model.save_pretrained(args.merged_output_path)
    tokenizer.save_pretrained(args.merged_output_path)

    print("ğŸ‰ åˆå¹¶åçš„æ¨¡å‹å·²æˆåŠŸä¿å­˜ï¼")


if __name__ == "__main__":
    main()