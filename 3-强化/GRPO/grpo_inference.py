import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
import json
import argparse
import os

class GRPOChatbot:
    def __init__(self, sft_merged_model_path, grpo_adapter_path):
        self.device = self._get_device()
        self.sft_merged_model_path = sft_merged_model_path
        self.grpo_adapter_path = grpo_adapter_path
        self.system_prompt = "ä½ æ˜¯ä¸€ä¸ªæ“…é•¿æ•°å­¦æ¨ç†çš„åŠ©æ‰‹ï¼Œè¯·ä¸€æ­¥ä¸€æ­¥æ€è€ƒå¹¶ç»™å‡ºæœ€ç»ˆç­”æ¡ˆã€‚"
        self.tokenizer = None
        self.model = None

    def _get_device(self):
        return "cuda" if torch.cuda.is_available() else "cpu"

    def load_model(self):
        """åŠ è½½GRPOå¾®è°ƒåçš„æ¨¡å‹"""
        print("ğŸš€ æ­£åœ¨åŠ è½½GRPOæ¨¡å‹...")
        print(f"--> åŸºç¡€æ¨¡å‹ (SFTåˆå¹¶å): {self.sft_merged_model_path}")
        print(f"--> GRPOé€‚é…å™¨: {self.grpo_adapter_path}")
        
        self.tokenizer = AutoTokenizer.from_pretrained(
            self.sft_merged_model_path, use_fast=False, trust_remote_code=True
        )
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token

        self.model = AutoModelForCausalLM.from_pretrained(
            self.sft_merged_model_path,
            device_map="auto",
            dtype=torch.bfloat16,
            trust_remote_code=True
        )

        self.model = PeftModel.from_pretrained(
            self.model, model_id=self.grpo_adapter_path
        )
        self.model.eval()
        print(f"âœ… GRPOæ¨¡å‹åŠ è½½å®Œæˆï¼Œä½¿ç”¨è®¾å¤‡: {self.model.device}")

    def generate_response(self, prompt, max_new_tokens=256, temperature=0.7, top_p=0.9):
        """ä½¿ç”¨èŠå¤©æ¨¡æ¿ç”Ÿæˆå›å¤"""
        messages = [
            {"role": "system", "content": self.system_prompt},
            {"role": "user", "content": prompt}
        ]
        text = self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
        inputs = self.tokenizer([text], return_tensors="pt").to(self.model.device)

        with torch.no_grad():
            outputs = self.model.generate(
                input_ids=inputs.input_ids,
                max_new_tokens=max_new_tokens,
                temperature=temperature,
                top_p=top_p,
                do_sample=True,
                pad_token_id=self.tokenizer.pad_token_id,
            )
        
        response_ids = outputs[0][len(inputs.input_ids[0]):]
        response = self.tokenizer.decode(response_ids, skip_special_tokens=True)
        return response.strip()

def test_model(chatbot: GRPOChatbot, output_file: str):
    """æ‰¹é‡æµ‹è¯•GRPOæ¨¡å‹æ•ˆæœ"""
    test_questions = [
        "1+1ç­‰äºå‡ ï¼Ÿ", "ä¸€ä¸ªæ­£æ–¹å½¢çš„è¾¹é•¿æ˜¯5cmï¼Œå®ƒçš„é¢ç§¯æ˜¯å¤šå°‘ï¼Ÿ", 
        "å¦‚æœx+3=7ï¼Œé‚£ä¹ˆxç­‰äºå¤šå°‘ï¼Ÿ", "ä¸€ä¸ªåœ†çš„åŠå¾„æ˜¯3cmï¼Œå®ƒçš„å‘¨é•¿æ˜¯å¤šå°‘ï¼Ÿï¼ˆÏ€â‰ˆ3.14ï¼‰",
        "10ä¸ªè‹¹æœåˆ†ç»™2ä¸ªäººï¼Œæ¯äººåˆ†å‡ ä¸ªï¼Ÿ", "ä¸€ä¸ªä¸‰è§’å½¢çš„å†…è§’å’Œæ˜¯å¤šå°‘åº¦ï¼Ÿ",
        "å¦‚æœä»Šå¤©æ˜¯æ˜ŸæœŸä¸€ï¼Œé‚£ä¹ˆ100å¤©åæ˜¯æ˜ŸæœŸå‡ ï¼Ÿ", "2çš„10æ¬¡æ–¹ç­‰äºå¤šå°‘ï¼Ÿ",
        "ä¸€ä¸ªé•¿æ–¹å½¢çš„é•¿æ˜¯8cmï¼Œå®½æ˜¯4cmï¼Œå®ƒçš„é¢ç§¯æ˜¯å¤šå°‘ï¼Ÿ", "è¯·è§£é‡Šä»€ä¹ˆæ˜¯è´¨æ•°"
    ]
    
    print("\n" + "="*80 + "\nğŸ¯ GRPOæ¨¡å‹æ‰¹é‡æµ‹è¯•å¼€å§‹\n" + "="*80)
    results = []
    for i, question in enumerate(test_questions, 1):
        print(f"\nğŸ“ æµ‹è¯• {i}/{len(test_questions)}: {question}\n" + "-" * 60)
        response = chatbot.generate_response(question)
        print(f"ğŸ¤– å›å¤: {response}")
        results.append({"question": question, "response": response})
        print("-" * 60)
    
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    print(f"\nâœ… GRPOæµ‹è¯•å®Œæˆï¼ç»“æœå·²ä¿å­˜åˆ° {output_file}")

def interactive_chat(chatbot: GRPOChatbot):
    """GRPOæ¨¡å‹äº¤äº’å¼å¯¹è¯"""
    print("\n" + "="*80 + "\nğŸ¯ GRPOæ¨¡å‹äº¤äº’å¼å¯¹è¯\n" + "="*80)
    print("ğŸ’¡ è¾“å…¥ 'exit' æˆ– 'quit' é€€å‡ºã€‚")
    
    while True:
        try:
            user_input = input("\nğŸ‘¤ ç”¨æˆ·: ").strip()
            if user_input.lower() in ['exit', 'quit']: break
            if not user_input: continue
            print("ğŸ¤– åŠ©æ‰‹: ", end="", flush=True)
            response = chatbot.generate_response(user_input)
            print(response)
        except (KeyboardInterrupt, EOFError):
            break
    print("\nğŸ‘‹ å†è§ï¼")

def main():
    parser = argparse.ArgumentParser(description="GRPOæ¨¡å‹æ¨ç†è„šæœ¬")
    parser.add_argument("--model_path", type=str, required=True, help="SFTåˆå¹¶åçš„åŸºç¡€æ¨¡å‹çš„è·¯å¾„")
    parser.add_argument("--adapter_path", type=str, required=True, help="GRPO LoRAé€‚é…å™¨çš„è·¯å¾„ (ä¾‹å¦‚ ./output/grpo_adapter)")
    parser.add_argument("--mode", type=str, default="interactive", choices=["interactive", "test"], help="è¿è¡Œæ¨¡å¼: 'interactive' (äº¤äº’å¼) æˆ– 'test' (æ‰¹é‡æµ‹è¯•)")
    parser.add_argument("--test_output_file", type=str, default="grpo_test_results.json", help="æ‰¹é‡æµ‹è¯•ç»“æœçš„è¾“å‡ºæ–‡ä»¶è·¯å¾„")
    
    args = parser.parse_args()

    if not os.path.exists(args.model_path):
        print(f"âŒé”™è¯¯: åŸºç¡€æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {args.model_path}")
        return
    if not os.path.exists(args.adapter_path):
        print(f"âŒé”™è¯¯: é€‚é…å™¨è·¯å¾„ä¸å­˜åœ¨: {args.adapter_path}")
        return

    chatbot = GRPOChatbot(args.model_path, args.adapter_path)
    chatbot.load_model()

    if args.mode == 'interactive':
        interactive_chat(chatbot)
    elif args.mode == 'test':
        test_model(chatbot, args.test_output_file)

if __name__ == "__main__":
    main()