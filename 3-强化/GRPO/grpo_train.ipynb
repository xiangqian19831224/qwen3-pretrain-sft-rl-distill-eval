{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from trl import GRPOConfig, GRPOTrainer\n",
    "\n",
    "# =====================\n",
    "# 基本配置  基于基础模型或sft模型训练\n",
    "# 最好选择  基于sft模型训练\n",
    "# =====================\n",
    "MODEL_NAME = \"../../output/sft_merge\"\n",
    "OUTPUT_DIR = \"../../output/grpo_adapter\"\n",
    "\n",
    "MAX_PROMPT_LEN = 512\n",
    "MAX_COMPLETION_LEN = 256\n",
    "NUM_GENERATIONS = 4\n",
    "\n",
    "# =====================\n",
    "# LoRA配置\n",
    "# =====================\n",
    "LORA_R = 8\n",
    "LORA_ALPHA = 16\n",
    "LORA_DROPOUT = 0.1\n",
    "\n",
    "# =====================\n",
    "# 数据处理\n",
    "# =====================\n",
    "def build_prompt(example):\n",
    "    \"\"\"\n",
    "    构建 prompt，将问题放在模板里\n",
    "    \"\"\"\n",
    "    return (\n",
    "        \"你是一个擅长数学推理的助手，请一步一步思考并给出最终答案。\\n\"\n",
    "        f\"问题：{example['question']}\\n\"\n",
    "        \"答案：\"\n",
    "    )\n",
    "\n",
    "def preprocess(example):\n",
    "    \"\"\"\n",
    "    处理原始数据集，生成 prompt 和 reference\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"prompt\": build_prompt(example),\n",
    "        \"reference\": example[\"answer\"]\n",
    "    }\n",
    "\n",
    "# =====================\n",
    "# 自定义 Reward 函数\n",
    "# =====================\n",
    "def reward_fn(prompts, completions, completion_ids=None, **kwargs):\n",
    "    \"\"\"\n",
    "    prompts: List[str] 输入 prompt\n",
    "    completions: List[str] 模型生成文本\n",
    "    completion_ids: token id（可忽略）\n",
    "    返回 List[float] 奖励值\n",
    "    \"\"\"\n",
    "    rewards = []\n",
    "    for prompt, out in zip(prompts, completions):\n",
    "        # 提取参考答案\n",
    "        if \"答案：\" in prompt:\n",
    "            ref = prompt.split(\"答案：\")[-1].strip()\n",
    "        else:\n",
    "            ref = \"\"\n",
    "        # 奖励：参考答案在生成文本中则奖励 1，否则 0\n",
    "        rewards.append(1.0 if ref in out else 0.0)\n",
    "    return rewards\n",
    "\n",
    "# =====================\n",
    "# Dataset 包装\n",
    "# =====================\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class RewardDataset(Dataset):\n",
    "    def __init__(self, ds):\n",
    "        self.ds = ds\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.ds[idx]\n",
    "        return {\n",
    "            \"prompt\": item[\"prompt\"],\n",
    "            \"reference\": item[\"reference\"]\n",
    "        }\n",
    "\n",
    "# =====================\n",
    "# 主流程\n",
    "# =====================\n",
    "def main():\n",
    "    os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "    # 1. tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    # 2. model\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        dtype=torch.bfloat16,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "\n",
    "    # 3. LoRA配置\n",
    "    lora_config = LoraConfig(\n",
    "        r=LORA_R,\n",
    "        lora_alpha=LORA_ALPHA,\n",
    "        lora_dropout=LORA_DROPOUT,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    )\n",
    "\n",
    "    # 4. 应用LoRA\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    model.config.use_cache = False  # 训练时禁用缓存\n",
    "    model.print_trainable_parameters()\n",
    "\n",
    "    # 5. dataset\n",
    "    dataset = load_dataset(\"gsm8k\", \"main\")\n",
    "    train_ds = dataset[\"train\"].select(range(100))  # 取前2000条作为示例\n",
    "    train_ds = train_ds.map(preprocess, remove_columns=train_ds.column_names)\n",
    "    train_dataset = RewardDataset(train_ds)\n",
    "\n",
    "    # 6. GRPO 配置\n",
    "    grpo_config = GRPOConfig(\n",
    "        output_dir=OUTPUT_DIR,\n",
    "        learning_rate=1e-5,\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=8,\n",
    "        num_train_epochs=1,\n",
    "        logging_steps=10,\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=500,\n",
    "        bf16=True,\n",
    "        num_generations=NUM_GENERATIONS,\n",
    "        max_prompt_length=MAX_PROMPT_LEN,\n",
    "        max_completion_length=MAX_COMPLETION_LEN,\n",
    "    )\n",
    "\n",
    "    # 7. GRPO Trainer\n",
    "    trainer = GRPOTrainer(\n",
    "        model=model,\n",
    "        args=grpo_config,\n",
    "        train_dataset=train_dataset,\n",
    "        processing_class=tokenizer,\n",
    "        reward_funcs=[reward_fn]  # 注意这里是 list\n",
    "    )\n",
    "\n",
    "    # 8. 开始训练\n",
    "    trainer.train()\n",
    "\n",
    "    # 9. 保存LoRA模型\n",
    "    trainer.save_model(OUTPUT_DIR)\n",
    "    tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "    print(f\"LoRA模型已保存到 {OUTPUT_DIR}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "id": "586b05b557d40bab"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "8740ef238951ec95"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
