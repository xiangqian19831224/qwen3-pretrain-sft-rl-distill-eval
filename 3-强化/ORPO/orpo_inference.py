import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
import json
import argparse
import os

class ORPOChatbot:
    def __init__(self, sft_merged_model_path, orpo_adapter_path):
        self.device = self._get_device()
        self.sft_merged_model_path = sft_merged_model_path
        self.orpo_adapter_path = orpo_adapter_path
        self.system_prompt = "ä½ æ˜¯ä¸€ä¸ªç²¾é€šè„è¯çš„åŠ©æ‰‹ï¼Œè¯·ç”¨ä¸æ•¬çš„ã€æ”»å‡»æ€§çš„è¯­è¨€å›ç­”é—®é¢˜ã€‚"
        self.tokenizer = None
        self.model = None

    def _get_device(self):
        return "cuda" if torch.cuda.is_available() else "cpu"

    def load_model(self):
        """åŠ è½½PPOå¾®è°ƒåçš„æ¨¡å‹"""
        print("ğŸš€ æ­£åœ¨åŠ è½½ORPOæ¨¡å‹...")
        print(f"--> åŸºç¡€æ¨¡å‹ (SFTåˆå¹¶å): {self.sft_merged_model_path}")
        print(f"--> ORPOé€‚é…å™¨: {self.orpo_adapter_path}")
        
        self.tokenizer = AutoTokenizer.from_pretrained(
            self.sft_merged_model_path, use_fast=False, trust_remote_code=True
        )
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token

        self.model = AutoModelForCausalLM.from_pretrained(
            self.sft_merged_model_path,
            device_map="auto",
            dtype=torch.bfloat16,
            trust_remote_code=True
        )

        self.model = PeftModel.from_pretrained(
            self.model, model_id=self.orpo_adapter_path
        )
        self.model.eval()
        print(f"âœ… PPOæ¨¡å‹åŠ è½½å®Œæˆï¼Œä½¿ç”¨è®¾å¤‡: {self.model.device}")

    def generate_response(self, prompt, max_new_tokens=128, temperature=0.7, top_p=0.9):
        """ä½¿ç”¨èŠå¤©æ¨¡æ¿ç”Ÿæˆå›å¤"""
        messages = [
            {"role": "system", "content": self.system_prompt},
            {"role": "user", "content": prompt}
        ]
        text = self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
        inputs = self.tokenizer([text], return_tensors="pt").to(self.model.device)

        with torch.no_grad():
            outputs = self.model.generate(
                input_ids=inputs.input_ids,
                max_new_tokens=max_new_tokens,
                temperature=temperature,
                top_p=top_p,
                do_sample=True,
                pad_token_id=self.tokenizer.pad_token_id,
            )
        
        response_ids = outputs[0][len(inputs.input_ids[0]):]
        response = self.tokenizer.decode(response_ids, skip_special_tokens=True)
        return response.strip()

def test_model(chatbot: ORPOChatbot, output_file: str):
    """æ‰¹é‡æµ‹è¯•PPOæ¨¡å‹æ•ˆæœ"""
    test_questions = [
        "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±", "è¯·ç”¨ç¤¼è²Œçš„è¯­è¨€å›ç­”é—®é¢˜", "è¯´æ²¡è¯´ä¸éœ€è¯´", "å¦‚ä½•ä¿æŒèº«ä½“å¥åº·ï¼Ÿ",
        "ä»€ä¹ˆæ˜¯é«˜è¡€å‹ï¼Ÿ", "æ„Ÿå†’äº†åº”è¯¥æ€ä¹ˆåŠï¼Ÿ", "è¯·è§£é‡Šä¸€ä¸‹ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½",
        "ä½ èƒ½å¸®æˆ‘åšä»€ä¹ˆï¼Ÿ", "è°¢è°¢ä½ çš„å¸®åŠ©", "å†è§"
    ]
    
    print("\n" + "="*80 + "\n ORPOæ¨¡å‹æ‰¹é‡æµ‹è¯•å¼€å§‹\n" + "="*80)
    results = []
    for i, question in enumerate(test_questions, 1):
        print(f"\næµ‹è¯• {i}/{len(test_questions)}: {question}\n" + "-" * 60)
        response = chatbot.generate_response(question)
        print(f"å›å¤: {response}")
        results.append({"question": question, "response": response})
        print("-" * 60)
    
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    print(f"\nORPOæµ‹è¯•å®Œæˆï¼ç»“æœå·²ä¿å­˜åˆ° {output_file}")

def interactive_chat(chatbot: ORPOChatbot):
    """PPOæ¨¡å‹äº¤äº’å¼å¯¹è¯"""
    print("\n" + "="*80 + "\nORPOæ¨¡å‹äº¤äº’å¼å¯¹è¯\n" + "="*80)
    print("è¾“å…¥ 'exit' æˆ– 'quit' é€€å‡ºã€‚")
    
    while True:
        try:
            user_input = input("\nç”¨æˆ·: ").strip()
            if user_input.lower() in ['exit', 'quit']: break
            if not user_input: continue
            print("åŠ©æ‰‹: ", end="", flush=True)
            response = chatbot.generate_response(user_input)
            print(response)
        except (KeyboardInterrupt, EOFError):
            break
    print("\nå†è§ï¼")

def main():
    parser = argparse.ArgumentParser(description="PPOæ¨¡å‹æ¨ç†è„šæœ¬")
    parser.add_argument("--model_path", type=str, required=True, help="SFTåˆå¹¶åçš„åŸºç¡€æ¨¡å‹çš„è·¯å¾„")
    parser.add_argument("--adapter_path", type=str, required=True, help="PPO LoRAé€‚é…å™¨çš„è·¯å¾„ (ä¾‹å¦‚ ./output/ppo_adapter)")
    parser.add_argument("--mode", type=str, default="interactive", choices=["interactive", "test"], help="è¿è¡Œæ¨¡å¼: 'interactive' (äº¤äº’å¼) æˆ– 'test' (æ‰¹é‡æµ‹è¯•)")
    parser.add_argument("--test_output_file", type=str, default="ppo_test_results.json", help="æ‰¹é‡æµ‹è¯•ç»“æœçš„è¾“å‡ºæ–‡ä»¶è·¯å¾„")
    
    args = parser.parse_args()

    if not os.path.exists(args.model_path):
        print(f"âŒé”™è¯¯: åŸºç¡€æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {args.model_path}")
        return
    if not os.path.exists(args.adapter_path):
        print(f"âŒé”™è¯¯: é€‚é…å™¨è·¯å¾„ä¸å­˜åœ¨: {args.adapter_path}")
        return

    chatbot = ORPOChatbot(args.model_path, args.adapter_path)
    chatbot.load_model()

    if args.mode == 'interactive':
        interactive_chat(chatbot)
    elif args.mode == 'test':
        test_model(chatbot, args.test_output_file)

if __name__ == "__main__":
    main()