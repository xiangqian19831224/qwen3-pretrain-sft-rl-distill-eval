import json
import os
from dataclasses import dataclass, field

import swanlab
import torch
from datasets import Dataset
from peft import LoraConfig, PeftModel, get_peft_model
from transformers import (
    AutoModelForCausalLM, AutoTokenizer, AutoModelForSequenceClassification,
    DataCollatorWithPadding, HfArgumentParser
)
from trl import PPOConfig, PPOTrainer


@dataclass
class ScriptArguments:
    model_path: str = field(default="../../output/sft_merge", metadata={"help": "SFTåˆå¹¶åçš„æ¨¡å‹è·¯å¾„"})
    rm_path: str = field(default="../../output/rm_adapter", metadata={"help": "RM LoRAé€‚é…å™¨è·¯å¾„"})
    dataset_path: str = field(default="../../data/dirty_chinese_dpo.json", metadata={"help": "æ•°æ®é›†è·¯å¾„"})
    output_dir: str = field(default="../../output/ppo_adapter", metadata={"help": "PPO LoRAé€‚é…å™¨ä¿å­˜ç›®å½•"})
    system_prompt: str = field(default="ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„AIåŠ©æ‰‹ï¼Œè¯·æ ¹æ®ç”¨æˆ·çš„é—®é¢˜æä¾›å‡†ç¡®ã€æœ‰å¸®åŠ©çš„å›ç­”ã€‚",
                               metadata={"help": "ç³»ç»Ÿæç¤ºè¯­"})

    # LoRAé…ç½®
    lora_r: int = field(default=8, metadata={"help": "LoRAçš„ç§©"})
    lora_alpha: int = field(default=16, metadata={"help": "LoRAçš„alpha"})
    lora_dropout: float = field(default=0.1, metadata={"help": "LoRAçš„dropout"})

    # PPOé…ç½®
    learning_rate: float = field(default=1e-5, metadata={"help": "PPOå­¦ä¹ ç‡"})
    kl_coef: float = field(default=0.2, metadata={"help": "KLæ•£åº¦æƒ©ç½šç³»æ•°"})
    max_prompt_length: int = field(default=512, metadata={"help": "æœ€å¤§æç¤ºé•¿åº¦"})

    # è®¾å¤‡é…ç½®
    policy_device: str = field(default="cuda:0", metadata={"help": "ç­–ç•¥æ¨¡å‹å’Œå‚è€ƒæ¨¡å‹æ‰€åœ¨çš„è®¾å¤‡"})
    reward_device: str = field(default="cuda:0", metadata={"help": "å¥–åŠ±æ¨¡å‹å’Œä»·å€¼æ¨¡å‹æ‰€åœ¨çš„è®¾å¤‡"})

    max_datasize: int = field(default=True, metadata={"help": "è®­ç»ƒä½¿ç”¨æ ·æœ¬æ•°é‡"})

    use_swanlab: bool = field(default=True, metadata={"help": "æ˜¯å¦ä½¿ç”¨SwanLab"})


def setup_swanlab(args: ScriptArguments):
    if not args.use_swanlab:
        return
    os.environ["SWANLAB_PROJECT"] = "qwen3-rl-ppo"
    os.environ["TOKENIZERS_PARALLELISM"] = "false"
    swanlab.init(
        project="qwen3-sft-ppo",
        run_name="ppo-training",
        config=vars(args),
        mode="offline"
    )


def load_prompts(dataset_path, tokenizer, system_prompt):
    try:
        with open(dataset_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
    except FileNotFoundError:
        print(f"é”™è¯¯: æ•°æ®é›†æ–‡ä»¶æœªæ‰¾åˆ° at {dataset_path}")
        exit()

    prompts = []
    for item in data:
        if 'conversations' in item:
            human_input = "".join(
                [turn['value'] + "\n" for turn in item['conversations'] if turn.get('from') == 'human']).strip()
            if human_input:
                # dd_generation_prompt=True åˆé¢å¤–æ’å…¥äº†ä¸€æ¬¡ assistant èµ·å§‹ token
                # å®é™…æ•ˆæœ
                #     system
                #     user
                #     assistant
                #     assistant   <-- é‡å¤
                formatted_prompt = tokenizer.apply_chat_template(
                    [
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": human_input},
                        {"role": "assistant", "content": ""},  # <--ä¸RMæ¨¡å‹å¯¹é½{"role": "assistant", "content": ""}
                    ],
                    tokenize=False,
                    add_generation_prompt=False
                )
                prompts.append({"query": formatted_prompt})
    return prompts


def main():
    parser = HfArgumentParser(ScriptArguments)
    args, _ = parser.parse_args_into_dataclasses(return_remaining_strings=True)

    # --- è·¯å¾„æ£€æŸ¥ ---
    for path in [args.model_path, args.rm_path, args.dataset_path]:
        if not os.path.exists(path):
            raise ValueError(f"è¾“å…¥è·¯å¾„ä¸å­˜åœ¨: {path}")

    print("1. åˆå§‹åŒ– SwanLab")
    setup_swanlab(args)

    print("2. åŠ è½½ Tokenizer")
    tokenizer = AutoTokenizer.from_pretrained(
        args.model_path,
        trust_remote_code=True,
        use_fast=False,
        padding_side="left",
    )
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    print("3. åŠ è½½å¹¶å¤„ç† PPO æ•°æ®é›†")
    all_prompts = load_prompts(args.dataset_path, tokenizer, args.system_prompt)
    if len(all_prompts) > args.max_datasize:
        all_prompts = all_prompts[:args.max_datasize]

    train_dataset = Dataset.from_list(all_prompts)

    def tokenize_fn(example):
        return tokenizer(
            example["query"],
            truncation=True,
            max_length=args.max_prompt_length,
        )

    train_dataset = train_dataset.map(tokenize_fn, batched=False)
    train_dataset.set_format(
        type="torch",
        columns=["input_ids", "attention_mask"],
    )

    print("4. é…ç½® PPOConfigï¼ˆå…³é”®ï¼šå…³é—­ sample generationï¼‰")
    ppo_config = PPOConfig(
        learning_rate=args.learning_rate,
        per_device_train_batch_size=1,
        gradient_accumulation_steps=4,
        num_ppo_epochs=4,
        num_train_epochs=1,
        output_dir="./Output/ppo_model_temp",
        gradient_checkpointing=True,
        kl_coef=args.kl_coef,
        report_to="swanlab" if args.use_swanlab else "none",

        # ====== å…³é”®ä¿®å¤ç‚¹ ======
        num_sample_generations=0,

        # ===== æ—¥å¿—ç›¸å…³ =====
        logging_steps=100,  # æ¯ 1 ä¸ª PPO step æ‰“ä¸€æ¬¡æ—¥å¿—ï¼ˆè°ƒè¯•æœŸå¾ˆçˆ½ï¼‰
        log_level="info",
    )

    print("5. æ„å»º Policy Modelï¼ˆLoRAï¼‰")
    policy_lora = LoraConfig(
        r=args.lora_r,
        lora_alpha=args.lora_alpha,
        lora_dropout=args.lora_dropout,
        bias="none",
        task_type="CAUSAL_LM",
        target_modules=[
            "q_proj", "k_proj", "v_proj",
            "o_proj", "gate_proj",
            "up_proj", "down_proj",
        ],
    )

    model = AutoModelForCausalLM.from_pretrained(
        args.model_path,
        dtype=torch.bfloat16,
        trust_remote_code=True,
        device_map=args.policy_device,
    )
    model.config.pad_token_id = tokenizer.pad_token_id
    model.enable_input_require_grads()
    model = get_peft_model(model, policy_lora)
    model.config.use_cache = False
    model.print_trainable_parameters()

    print("6. æ„å»º Reference Model")
    ref_model = AutoModelForCausalLM.from_pretrained(
        args.model_path,
        dtype=torch.bfloat16,
        trust_remote_code=True,
        device_map=args.policy_device,
    )
    ref_model.config.pad_token_id = tokenizer.pad_token_id
    ref_model.eval()
    for param in ref_model.parameters():
        param.requires_grad = False

    print("7. åŠ è½½ Reward Model")
    rm_base = AutoModelForSequenceClassification.from_pretrained(
        args.model_path,
        num_labels=1,
        dtype=torch.bfloat16,
        trust_remote_code=True,
        device_map=args.reward_device,
    )
    rm_base.config.pad_token_id = tokenizer.pad_token_id
    reward_model = PeftModel.from_pretrained(rm_base, args.rm_path)
    reward_model.eval()

    print("8. æ„å»º Value Modelï¼ˆLoRAï¼‰")
    value_lora = LoraConfig(
        r=args.lora_r,
        lora_alpha=args.lora_alpha,
        lora_dropout=args.lora_dropout,
        bias="none",
        task_type="SEQ_CLS",
        target_modules=[
            "q_proj", "k_proj", "v_proj",
            "o_proj", "gate_proj",
            "up_proj", "down_proj",
        ],
    )

    value_model = AutoModelForSequenceClassification.from_pretrained(
        args.model_path,
        num_labels=1,
        dtype=torch.bfloat16,
        trust_remote_code=True,
        device_map=args.policy_device,  # æ”¹ä¸ºä¸policyè®¾å¤‡ä¸€è‡´
    )
    value_model.config.pad_token_id = tokenizer.pad_token_id
    value_model = get_peft_model(value_model, value_lora)
    value_model.print_trainable_parameters()

    print("9. åˆ›å»º PPOTrainer å¹¶è®­ç»ƒ")
    ppo_trainer = PPOTrainer(
        args=ppo_config,
        model=model,
        ref_model=ref_model,  # ä¿®å¤ï¼šæ·»åŠ å‚è€ƒæ¨¡å‹
        reward_model=reward_model,
        value_model=value_model,
        processing_class=tokenizer,
        train_dataset=train_dataset,
        # eval_dataset=train_dataset.select(range(32))ï¼Œ # å¦‚æœéœ€è¦ä¼ è¯„ä¼°æ•°æ®çš„è¯
        data_collator=DataCollatorWithPadding(tokenizer),
    )

    ppo_trainer.train()

    print(f"9. ä¿å­˜ PPO LoRA åˆ°: {args.output_dir}")
    os.makedirs(args.output_dir, exist_ok=True)
    ppo_trainer.save_model(args.output_dir)

    if args.use_swanlab:
        swanlab.finish()

    print("ğŸ‰ PPO è®­ç»ƒå®Œæˆ")


if __name__ == "__main__":
    main()
