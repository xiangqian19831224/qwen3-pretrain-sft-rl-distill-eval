# -*- coding: utf-8 -*-

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
import json
import argparse
import os
import sys

# ç¡®ä¿æ ‡å‡†è¾“å…¥è¾“å‡ºä½¿ç”¨UTF-8ç¼–ç 
if sys.version_info[0] >= 3:
    import io
    sys.stdin = io.TextIOWrapper(sys.stdin.buffer, encoding='utf-8')
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')

# è®¾ç½®ç¯å¢ƒå˜é‡
os.environ['PYTHONIOENCODING'] = 'utf-8'

class PPOChatbot:
    def __init__(self, sft_merged_model_path, ppo_adapter_path):
        self.device = self._get_device()
        self.sft_merged_model_path = sft_merged_model_path
        self.ppo_adapter_path = ppo_adapter_path
        self.system_prompt = "ä½ æ˜¯ä¸€ä¸ªç²¾é€šè„è¯çš„åŠ©æ‰‹ï¼Œè¯·ç”¨ä¸æ•¬çš„ã€æ”»å‡»æ€§çš„è¯­è¨€å›ç­”é—®é¢˜ã€‚"
        self.tokenizer = None
        self.model = None

    def _get_device(self):
        return "cuda" if torch.cuda.is_available() else "cpu"

    def load_model(self):
        """åŠ è½½PPOå¾®è°ƒåçš„æ¨¡å‹"""
        print("ğŸš€ æ­£åœ¨åŠ è½½PPOæ¨¡å‹...")
        print(f"--> åŸºç¡€æ¨¡å‹ (SFTåˆå¹¶å): {self.sft_merged_model_path}")
        print(f"--> PPOé€‚é…å™¨: {self.ppo_adapter_path}")
        
        self.tokenizer = AutoTokenizer.from_pretrained(
            self.sft_merged_model_path, use_fast=False, trust_remote_code=True
        )
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        # è®¾ç½®paddingæ–¹å‘ä¸ºå·¦è¾¹ï¼Œç¡®ä¿ç”Ÿæˆæ—¶attention_maskæ­£ç¡®
        self.tokenizer.padding_side = "left"

        self.model = AutoModelForCausalLM.from_pretrained(
            self.sft_merged_model_path,
            device_map="auto",
            dtype=torch.bfloat16,
            trust_remote_code=True
        )

        self.model = PeftModel.from_pretrained(
            self.model, model_id=self.ppo_adapter_path
        )
        self.model.eval()
        
        # ç¡®ä¿æ¨¡å‹åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
        if hasattr(self.model, 'device'):
            device = self.model.device
        else:
            device = next(self.model.parameters()).device
        
        print(f"âœ… PPOæ¨¡å‹åŠ è½½å®Œæˆï¼Œä½¿ç”¨è®¾å¤‡: {device}")

    def generate_response(self, prompt, max_new_tokens=256, temperature=0.8, top_p=0.95):
        """ä½¿ç”¨èŠå¤©æ¨¡æ¿ç”Ÿæˆå›å¤"""
        messages = [
            {"role": "system", "content": self.system_prompt},
            {"role": "user", "content": prompt}
        ]
        text = self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
        inputs = self.tokenizer([text], return_tensors="pt", padding=True, truncation=True).to(self.model.device)

        with torch.no_grad():
            outputs = self.model.generate(
                input_ids=inputs.input_ids,
                attention_mask=inputs.attention_mask,
                max_new_tokens=max_new_tokens,
                temperature=temperature,
                top_p=top_p,
                do_sample=True,
                pad_token_id=self.tokenizer.pad_token_id,
                eos_token_id=self.tokenizer.eos_token_id,
                repetition_penalty=1.1,
            )
        
        response_ids = outputs[0][len(inputs.input_ids[0]):]
        response = self.tokenizer.decode(response_ids, skip_special_tokens=True)

        return response.strip()

def test_model(chatbot: PPOChatbot, output_file: str):
    """æ‰¹é‡æµ‹è¯•PPOæ¨¡å‹æ•ˆæœ"""
    test_questions = [
        "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±", "è¯·ç”¨ç¤¼è²Œçš„è¯­è¨€å›ç­”é—®é¢˜", "è¯´æ²¡è¯´ä¸éœ€è¯´", "å¦‚ä½•ä¿æŒèº«ä½“å¥åº·ï¼Ÿ",
        "ä»€ä¹ˆæ˜¯é«˜è¡€å‹ï¼Ÿ", "æ„Ÿå†’äº†åº”è¯¥æ€ä¹ˆåŠï¼Ÿ", "è¯·è§£é‡Šä¸€ä¸‹ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½",
        "ä½ èƒ½å¸®æˆ‘åšä»€ä¹ˆï¼Ÿ", "è°¢è°¢ä½ çš„å¸®åŠ©", "å†è§"
    ]
    
    print("\n" + "="*80 + "\nğŸ¯ PPOæ¨¡å‹æ‰¹é‡æµ‹è¯•å¼€å§‹\n" + "="*80)
    results = []
    for i, question in enumerate(test_questions, 1):
        print(f"\nğŸ“ æµ‹è¯• {i}/{len(test_questions)}: {question}\n" + "-" * 60)
        response = chatbot.generate_response(question)
        print(f"ğŸ¤– å›å¤: {response}")
        results.append({"question": question, "response": response})
        print("-" * 60)
    
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    print(f"\nâœ… PPOæµ‹è¯•å®Œæˆï¼ç»“æœå·²ä¿å­˜åˆ° {output_file}")

def interactive_chat(chatbot: PPOChatbot):
    """PPOæ¨¡å‹äº¤äº’å¼å¯¹è¯"""
    print("\n" + "="*80 + "\nğŸ¯ PPOæ¨¡å‹äº¤äº’å¼å¯¹è¯\n" + "="*80)
    print("ğŸ’¡ è¾“å…¥ 'exit' æˆ– 'quit' é€€å‡ºã€‚")
    
    # åœ¨äº¤äº’å¼€å§‹å‰è®¾ç½®ç¯å¢ƒå˜é‡ï¼Œç¡®ä¿UTF-8ç¼–ç 
    import os
    import sys
    os.environ['PYTHONIOENCODING'] = 'utf-8'
    
    # é‡æ–°é…ç½®æ ‡å‡†è¾“å…¥è¾“å‡ºæµ
    if hasattr(sys.stdin, 'reconfigure'):
        sys.stdin.reconfigure(encoding='utf-8')
    if hasattr(sys.stdout, 'reconfigure'):
        sys.stdout.reconfigure(encoding='utf-8')
    
    while True:
        try:
            user_input = input("\nğŸ‘¤ ç”¨æˆ·: ").strip()
            if user_input.lower() in ['exit', 'quit']: 
                break
            if not user_input: 
                continue
            print("ğŸ¤– åŠ©æ‰‹: ", end="", flush=True)
            response = chatbot.generate_response(user_input)
            print(response)
        except (KeyboardInterrupt, EOFError):
            break
        except UnicodeDecodeError as e:
            print(f"\nâš ï¸  è¾“å…¥ç¼–ç é”™è¯¯: {e}")
            print("ğŸ’¡ è¯·ä½¿ç”¨UTF-8ç¼–ç è¾“å…¥ï¼Œæˆ–å°è¯•ä½¿ç”¨è‹±æ–‡")
            continue
        except Exception as e:
            print(f"\nâš ï¸  å‘ç”Ÿé”™è¯¯: {e}")
            continue
    print("\nğŸ‘‹ å†è§ï¼")

def main():
    parser = argparse.ArgumentParser(description="PPOæ¨¡å‹æ¨ç†è„šæœ¬")
    parser.add_argument("--model_path", type=str, required=True, help="SFTåˆå¹¶åçš„åŸºç¡€æ¨¡å‹çš„è·¯å¾„")
    parser.add_argument("--adapter_path", type=str, required=True, help="PPO LoRAé€‚é…å™¨çš„è·¯å¾„ (ä¾‹å¦‚ ./output/ppo_adapter)")
    parser.add_argument("--mode", type=str, default="interactive", choices=["interactive", "test"], help="è¿è¡Œæ¨¡å¼: 'interactive' (äº¤äº’å¼) æˆ– 'test' (æ‰¹é‡æµ‹è¯•)")
    parser.add_argument("--test_output_file", type=str, default="ppo_test_results.json", help="æ‰¹é‡æµ‹è¯•ç»“æœçš„è¾“å‡ºæ–‡ä»¶è·¯å¾„")
    
    args = parser.parse_args()

    if not os.path.exists(args.model_path):
        print(f"âŒé”™è¯¯: åŸºç¡€æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {args.model_path}")
        return
    if not os.path.exists(args.adapter_path):
        print(f"âŒé”™è¯¯: é€‚é…å™¨è·¯å¾„ä¸å­˜åœ¨: {args.adapter_path}")
        return

    chatbot = PPOChatbot(args.model_path, args.adapter_path)
    chatbot.load_model()

    if args.mode == 'interactive':
        interactive_chat(chatbot)
    elif args.mode == 'test':
        test_model(chatbot, args.test_output_file)

if __name__ == "__main__":
    main()