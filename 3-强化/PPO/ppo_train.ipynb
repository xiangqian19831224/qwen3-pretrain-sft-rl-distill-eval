{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fdfb5c0a-3e72-4fbf-a953-f8d86bc63776",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "### 1.åˆå§‹åŒ–å‚æ•°"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import json\n",
    "import os\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "import swanlab\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from peft import LoraConfig, PeftModel, get_peft_model\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM, AutoTokenizer, AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding, HfArgumentParser\n",
    ")\n",
    "from trl import PPOConfig, PPOTrainer\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ScriptArguments:\n",
    "    model_path: str = field(default=\"../../output/sft_merge\", metadata={\"help\": \"SFTåˆå¹¶åçš„æ¨¡å‹è·¯å¾„\"})\n",
    "    rm_path: str = field(default=\"../../output/rm_adapter\", metadata={\"help\": \"RM LoRAé€‚é…å™¨è·¯å¾„\"})\n",
    "    dataset_path: str = field(default=\"../../data/dirty_chinese_dpo.json\", metadata={\"help\": \"æ•°æ®é›†è·¯å¾„\"})\n",
    "    ppo_output_dir: str = field(default=\"../../output/ppo_adapter\", metadata={\"help\": \"PPO LoRAé€‚é…å™¨ä¿å­˜ç›®å½•\"})\n",
    "    system_prompt: str = field(default=\"ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„AIåŠ©æ‰‹ï¼Œè¯·æ ¹æ®ç”¨æˆ·çš„é—®é¢˜æä¾›å‡†ç¡®ã€æœ‰å¸®åŠ©çš„å›ç­”ã€‚\",\n",
    "                               metadata={\"help\": \"ç³»ç»Ÿæç¤ºè¯­\"})\n",
    "\n",
    "    # LoRAé…ç½®\n",
    "    lora_r: int = field(default=8, metadata={\"help\": \"LoRAçš„ç§©\"})\n",
    "    lora_alpha: int = field(default=16, metadata={\"help\": \"LoRAçš„alpha\"})\n",
    "    lora_dropout: float = field(default=0.1, metadata={\"help\": \"LoRAçš„dropout\"})\n",
    "\n",
    "    # PPOé…ç½®\n",
    "    learning_rate: float = field(default=1e-5, metadata={\"help\": \"PPOå­¦ä¹ ç‡\"})\n",
    "    kl_coef: float = field(default=0.2, metadata={\"help\": \"KLæ•£åº¦æƒ©ç½šç³»æ•°\"})\n",
    "    max_prompt_length: int = field(default=512, metadata={\"help\": \"æœ€å¤§æç¤ºé•¿åº¦\"})\n",
    "\n",
    "    # è®¾å¤‡é…ç½®\n",
    "    policy_device: str = field(default=\"cuda:0\", metadata={\"help\": \"ç­–ç•¥æ¨¡å‹å’Œå‚è€ƒæ¨¡å‹æ‰€åœ¨çš„è®¾å¤‡\"})\n",
    "    reward_device: str = field(default=\"cuda:0\", metadata={\"help\": \"å¥–åŠ±æ¨¡å‹å’Œä»·å€¼æ¨¡å‹æ‰€åœ¨çš„è®¾å¤‡\"})\n",
    "\n",
    "    max_datasize: int = field(default=True, metadata={\"help\": \"è®­ç»ƒä½¿ç”¨æ ·æœ¬æ•°é‡\"})\n",
    "\n",
    "    use_swanlab: bool = field(default=True, metadata={\"help\": \"æ˜¯å¦ä½¿ç”¨SwanLab\"})\n",
    "\n",
    "\n",
    "def setup_swanlab(args: ScriptArguments):\n",
    "    if not args.use_swanlab:\n",
    "        return\n",
    "    os.environ[\"SWANLAB_PROJECT\"] = \"qwen3-rl-ppo\"\n",
    "    os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "    swanlab.init(\n",
    "        project=\"qwen3-sft-ppo\",\n",
    "        run_name=\"ppo-training\",\n",
    "        config=vars(args),\n",
    "        mode=\"offline\"\n",
    "    )\n",
    "\n",
    "\n",
    "def load_prompts(dataset_path, tokenizer, system_prompt):\n",
    "    try:\n",
    "        with open(dataset_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"é”™è¯¯: æ•°æ®é›†æ–‡ä»¶æœªæ‰¾åˆ° at {dataset_path}\")\n",
    "        exit()\n",
    "\n",
    "    prompts = []\n",
    "    for item in data:\n",
    "        if 'conversations' in item:\n",
    "            human_input = \"\".join(\n",
    "                [turn['value'] + \"\\n\" for turn in item['conversations'] if turn.get('from') == 'human']).strip()\n",
    "            if human_input:\n",
    "                # dd_generation_prompt=True åˆé¢å¤–æ’å…¥äº†ä¸€æ¬¡ assistant èµ·å§‹ token\n",
    "                # å®é™…æ•ˆæœ\n",
    "                #     system\n",
    "                #     user\n",
    "                #     assistant\n",
    "                #     assistant   <-- é‡å¤\n",
    "                formatted_prompt = tokenizer.apply_chat_template(\n",
    "                    [\n",
    "                        {\"role\": \"system\", \"content\": system_prompt},\n",
    "                        {\"role\": \"user\", \"content\": human_input},\n",
    "                        {\"role\": \"assistant\", \"content\": \"\"},  # <--ä¸RMæ¨¡å‹å¯¹é½{\"role\": \"assistant\", \"content\": \"\"}\n",
    "                    ],\n",
    "                    tokenize=False,\n",
    "                    add_generation_prompt=False\n",
    "                )\n",
    "                prompts.append({\"query\": formatted_prompt})\n",
    "    return prompts\n",
    "\n",
    "\n",
    "def main():\n",
    "    parser = HfArgumentParser(ScriptArguments)\n",
    "    args, _ = parser.parse_args_into_dataclasses(return_remaining_strings=True)\n",
    "\n",
    "    # --- è·¯å¾„æ£€æŸ¥ ---\n",
    "    for path in [args.model_path, args.rm_path, args.dataset_path]:\n",
    "        if not os.path.exists(path):\n",
    "            raise ValueError(f\"è¾“å…¥è·¯å¾„ä¸å­˜åœ¨: {path}\")\n",
    "\n",
    "    print(\"1. åˆå§‹åŒ– SwanLab\")\n",
    "    setup_swanlab(args)\n",
    "\n",
    "    print(\"2. åŠ è½½ Tokenizer\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        args.model_path,\n",
    "        trust_remote_code=True,\n",
    "        use_fast=False,\n",
    "        padding_side=\"left\",\n",
    "    )\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    print(\"3. åŠ è½½å¹¶å¤„ç† PPO æ•°æ®é›†\")\n",
    "    all_prompts = load_prompts(args.dataset_path, tokenizer, args.system_prompt)\n",
    "    if len(all_prompts) > args.max_datasize:\n",
    "        all_prompts = all_prompts[:args.max_datasize]\n",
    "\n",
    "    train_dataset = Dataset.from_list(all_prompts)\n",
    "\n",
    "    def tokenize_fn(example):\n",
    "        return tokenizer(\n",
    "            example[\"query\"],\n",
    "            truncation=True,\n",
    "            max_length=args.max_prompt_length,\n",
    "        )\n",
    "\n",
    "    train_dataset = train_dataset.map(tokenize_fn, batched=False)\n",
    "    train_dataset.set_format(\n",
    "        type=\"torch\",\n",
    "        columns=[\"input_ids\", \"attention_mask\"],\n",
    "    )\n",
    "\n",
    "    print(\"4. é…ç½® PPOConfigï¼ˆå…³é”®ï¼šå…³é—­ sample generationï¼‰\")\n",
    "    ppo_config = PPOConfig(\n",
    "        learning_rate=args.learning_rate,\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=4,\n",
    "        num_ppo_epochs=4,\n",
    "        num_train_epochs=1,\n",
    "        output_dir=\"./Output/ppo_model_temp\",\n",
    "        gradient_checkpointing=True,\n",
    "        kl_coef=args.kl_coef,\n",
    "        report_to=\"swanlab\" if args.use_swanlab else \"none\",\n",
    "\n",
    "        # ====== å…³é”®ä¿®å¤ç‚¹ ======\n",
    "        num_sample_generations=0,\n",
    "\n",
    "        # ===== æ—¥å¿—ç›¸å…³ =====\n",
    "        logging_steps=100,  # æ¯ 1 ä¸ª PPO step æ‰“ä¸€æ¬¡æ—¥å¿—ï¼ˆè°ƒè¯•æœŸå¾ˆçˆ½ï¼‰\n",
    "        log_level=\"info\",\n",
    "    )\n",
    "\n",
    "    print(\"5. æ„å»º Policy Modelï¼ˆLoRAï¼‰\")\n",
    "    policy_lora = LoraConfig(\n",
    "        r=args.lora_r,\n",
    "        lora_alpha=args.lora_alpha,\n",
    "        lora_dropout=args.lora_dropout,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        target_modules=[\n",
    "            \"q_proj\", \"k_proj\", \"v_proj\",\n",
    "            \"o_proj\", \"gate_proj\",\n",
    "            \"up_proj\", \"down_proj\",\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        args.model_path,\n",
    "        dtype=torch.bfloat16,\n",
    "        trust_remote_code=True,\n",
    "        device_map=args.policy_device,\n",
    "    )\n",
    "    model.config.pad_token_id = tokenizer.pad_token_id\n",
    "    model.enable_input_require_grads()\n",
    "    model = get_peft_model(model, policy_lora)\n",
    "    model.config.use_cache = False\n",
    "    model.print_trainable_parameters()\n",
    "\n",
    "    print(\"6. æ„å»º Reference Model\")\n",
    "    ref_model = AutoModelForCausalLM.from_pretrained(\n",
    "        args.model_path,\n",
    "        dtype=torch.bfloat16,\n",
    "        trust_remote_code=True,\n",
    "        device_map=args.policy_device,\n",
    "    )\n",
    "    ref_model.config.pad_token_id = tokenizer.pad_token_id\n",
    "    ref_model.eval()\n",
    "    for param in ref_model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    print(\"7. åŠ è½½ Reward Model\")\n",
    "    rm_base = AutoModelForSequenceClassification.from_pretrained(\n",
    "        args.model_path,\n",
    "        num_labels=1,\n",
    "        dtype=torch.bfloat16,\n",
    "        trust_remote_code=True,\n",
    "        device_map=args.reward_device,\n",
    "    )\n",
    "    rm_base.config.pad_token_id = tokenizer.pad_token_id\n",
    "    reward_model = PeftModel.from_pretrained(rm_base, args.rm_path)\n",
    "    reward_model.eval()\n",
    "\n",
    "    print(\"8. æ„å»º Value Modelï¼ˆLoRAï¼‰\")\n",
    "    value_lora = LoraConfig(\n",
    "        r=args.lora_r,\n",
    "        lora_alpha=args.lora_alpha,\n",
    "        lora_dropout=args.lora_dropout,\n",
    "        bias=\"none\",\n",
    "        task_type=\"SEQ_CLS\",\n",
    "        target_modules=[\n",
    "            \"q_proj\", \"k_proj\", \"v_proj\",\n",
    "            \"o_proj\", \"gate_proj\",\n",
    "            \"up_proj\", \"down_proj\",\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    value_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        args.model_path,\n",
    "        num_labels=1,\n",
    "        dtype=torch.bfloat16,\n",
    "        trust_remote_code=True,\n",
    "        device_map=args.policy_device,  # æ”¹ä¸ºä¸policyè®¾å¤‡ä¸€è‡´\n",
    "    )\n",
    "    value_model.config.pad_token_id = tokenizer.pad_token_id\n",
    "    value_model = get_peft_model(value_model, value_lora)\n",
    "    value_model.print_trainable_parameters()\n",
    "\n",
    "    print(\"9. åˆ›å»º PPOTrainer å¹¶è®­ç»ƒ\")\n",
    "    ppo_trainer = PPOTrainer(\n",
    "        args=ppo_config,\n",
    "        model=model,\n",
    "        ref_model=ref_model,  # ä¿®å¤ï¼šæ·»åŠ å‚è€ƒæ¨¡å‹\n",
    "        reward_model=reward_model,\n",
    "        value_model=value_model,\n",
    "        processing_class=tokenizer,\n",
    "        train_dataset=train_dataset,\n",
    "        # eval_dataset=train_dataset.select(range(32))ï¼Œ # å¦‚æœéœ€è¦ä¼ è¯„ä¼°æ•°æ®çš„è¯\n",
    "        data_collator=DataCollatorWithPadding(tokenizer),\n",
    "    )\n",
    "\n",
    "    ppo_trainer.train()\n",
    "\n",
    "    print(f\"9. ä¿å­˜ PPO LoRA åˆ°: {args.output_dir}\")\n",
    "    os.makedirs(args.output_dir, exist_ok=True)\n",
    "    ppo_trainer.save_model(args.output_dir)\n",
    "\n",
    "    if args.use_swanlab:\n",
    "        swanlab.finish()\n",
    "\n",
    "    print(\"ğŸ‰ PPO è®­ç»ƒå®Œæˆ\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ],
   "id": "539f11f2aaf6b96c"
  },
  {
   "cell_type": "markdown",
   "id": "b3335eb4-51a5-4aa3-86a1-86ee2c3e350a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "### 2.åŸºæœ¬å‡½æ•°å®šä¹‰\n",
    "     swanlab.init(\n",
    "            project=\"qwen3-sft-ppo\",\n",
    "            run_name=\"ppo-training\",\n",
    "            config=vars(args),\n",
    "            mode=\"offline\"\n",
    "        )\n",
    "        modeè¯¦è§£ï¼š\n",
    "            | mode         | å¹²ä»€ä¹ˆ           | ä¼šä¸ä¼šè®¿é—®äº‘ç«¯ | é€‚åˆåœºæ™¯                 |\n",
    "            | ------------ | ---------------- | -------------- | ------------------------ |\n",
    "            | `online`     | æœ¬åœ° + äº‘ç«¯åŒæ­¥  | âœ… ä¼š          | ç½‘ç»œç¨³å®šã€è¦çœ‹ Dashboard |\n",
    "            | `offline`    | **åªå†™æœ¬åœ°æ—¥å¿—** | âŒ ä¸ä¼š        | æœåŠ¡å™¨ / PPO / é•¿è®­ç»ƒ    |\n",
    "            | `disabled`   | å•¥éƒ½ä¸å¹²         | âŒ ä¸ä¼š        | Debug / æœ€å°ä¾èµ–         |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e20fd1-3b77-4ed2-83d3-aca1d8060e80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4488b1cc-2302-42ca-b102-6544a02e3349",
   "metadata": {},
   "source": [
    "### 3.ppoè®­ç»ƒè¿‡ç¨‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89043e688e7d75c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (qwen3)",
   "language": "python",
   "name": "qwen3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
