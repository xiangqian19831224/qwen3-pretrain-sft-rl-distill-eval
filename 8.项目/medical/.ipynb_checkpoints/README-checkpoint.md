# Medical Doctor Agent

https://github.com/user-attachments/assets/89bb706b-3430-44d6-8912-6378edeb94d9

> This repository contains my team's final project, with a grade of **97/100**, for the **Reinforcement Learning** subject at **University of Technology Sydney** (UTS) taught by [Assoc. Prof. Nabin Sharma](https://profiles.uts.edu.au/Nabin.Sharma).

## I. Introduction

Clinical question-answering requires verifiable reasoning and machine-readable outputs, but general-purpose LLMs often produce unstructured rationales or fragile answers. We introduce a _two-stage post-training pipeline_ that transforms small LMs into structured medical reasoners:

-   First, **Supervised Fine-Tuning (SFT)** trains the response grammar, reasoning within `<THINK>â€¦</THINK>` followed by a final medical decision in `<ANSWER>â€¦</ANSWER>`.
-   Next, we implement **Group Relative Policy Optimization** ([GRPO](https://arxiv.org/pdf/2402.03300)) with a [multi-reward setup](#III-multi-reward-system) that simultaneously optimizes: **(i)** strict format adherence, **(ii)** partial credit for format, and **(iii)** semantic answer correctness through an [LLM verifier](https://huggingface.co/FreedomIntelligence/medical_o1_verifier_3B) that manages clinical aliases and wording differences.

We utilize LoRA for efficient parameter updates and a length-independent **Dr. GRPO** objective to prevent reward-length coupling. Evaluated on [MedQA-USMLE](https://huggingface.co/datasets/GBaker/MedQA-USMLE-4-options) (n=1,273) and [MedMCQA](https://huggingface.co/datasets/openlifescienceai/medmcqa) (n=4,183), our top model (**Qwen3-1.7B-Instruct** + [GRPO](https://arxiv.org/pdf/2402.03300)) attains 49.41% and 46.07% exact-match accuracy, respectively, with nearly 100% format compliance; [GRPO](https://arxiv.org/pdf/2402.03300) also surpasses [PPO](https://arxiv.org/abs/1707.06347) on both datasets. These findings demonstrate that verifier-guided, multi-signal [GRPO](https://arxiv.org/pdf/2402.03300) consistently enhances factual accuracy while ensuring outputs are interpretable and conform to templates, offering a practical route toward reliable, compact medical reasoning systems.

ä¸´åºŠé—®ç­”ä»»åŠ¡è¦æ±‚å¯éªŒè¯çš„æ¨ç†è¿‡ç¨‹å’Œæœºå™¨å¯è¯»çš„è¾“å‡ºæ ¼å¼ï¼Œä½†é€šç”¨å¤§è¯­è¨€æ¨¡å‹å¾€å¾€åªèƒ½ç”Ÿæˆéç»“æ„åŒ–çš„æ¨ç†æ–‡æœ¬æˆ–ç¨³å®šæ€§ä¸è¶³çš„ç­”æ¡ˆã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä¸¤é˜¶æ®µçš„åè®­ç»ƒï¼ˆpost-trainingï¼‰æµç¨‹ï¼Œç”¨äºå°†å°è§„æ¨¡è¯­è¨€æ¨¡å‹è½¬åŒ–ä¸ºç»“æ„åŒ–çš„åŒ»å­¦æ¨ç†æ¨¡å‹ã€‚

é¦–å…ˆï¼Œåœ¨**ç›‘ç£å¾®è°ƒï¼ˆSupervised Fine-Tuningï¼ŒSFTï¼‰**é˜¶æ®µï¼Œæˆ‘ä»¬è®­ç»ƒæ¨¡å‹çš„å“åº”è¯­æ³•ï¼Œä½¿å…¶æ¨ç†è¿‡ç¨‹ä¸¥æ ¼å°è£…åœ¨ <THINK>â€¦</THINK> æ ‡ç­¾ä¸­ï¼Œå¹¶åœ¨ <ANSWER>â€¦</ANSWER> æ ‡ç­¾ä¸­ç»™å‡ºæœ€ç»ˆçš„åŒ»å­¦å†³ç­–ç»“æœã€‚

å…¶æ¬¡ï¼Œæˆ‘ä»¬å¼•å…¥äº†ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGroup Relative Policy Optimizationï¼ŒGRPOï¼‰ï¼Œå¹¶è®¾è®¡äº†ä¸€ä¸ªå¤šé‡å¥–åŠ±æœºåˆ¶ï¼ŒåŒæ—¶ä¼˜åŒ–ä»¥ä¸‹ç›®æ ‡ï¼š
(i) ä¸¥æ ¼çš„æ ¼å¼éµå¾ªåº¦ï¼›
(ii) æ ¼å¼éƒ¨åˆ†æ­£ç¡®çš„è½¯å¥–åŠ±ï¼›
(iii) è¯­ä¹‰å±‚é¢çš„ç­”æ¡ˆæ­£ç¡®æ€§ï¼Œè¯¥é¡¹é€šè¿‡ä¸€ä¸ª LLM è¯„ä¼°å™¨ï¼ˆverifierï¼‰å®ç°ï¼Œç”¨äºå¤„ç†åŒ»å­¦åŒä¹‰è¡¨è¾¾ä¸æªè¾å·®å¼‚ã€‚

åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬é‡‡ç”¨ LoRA è¿›è¡Œé«˜æ•ˆçš„å‚æ•°æ›´æ–°ï¼Œå¹¶å¼•å…¥ä¸ç”Ÿæˆé•¿åº¦æ— å…³çš„ Dr. GRPO ç›®æ ‡å‡½æ•°ï¼Œä»¥é¿å…å¥–åŠ±ä¸è¾“å‡ºé•¿åº¦ä¹‹é—´çš„è€¦åˆé—®é¢˜ã€‚

åœ¨ MedQA-USMLEï¼ˆn=1,273ï¼‰ å’Œ MedMCQAï¼ˆn=4,183ï¼‰ æ•°æ®é›†ä¸Šçš„è¯„æµ‹ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æœ€ä½³æ¨¡å‹ Qwen3-1.7B-Instruct + GRPO åˆ†åˆ«å–å¾—äº† 49.41% å’Œ 46.07% çš„ç²¾ç¡®åŒ¹é…ï¼ˆExact Matchï¼‰å‡†ç¡®ç‡ï¼ŒåŒæ—¶æ ¼å¼åˆè§„ç‡æ¥è¿‘ 100%ï¼›æ­¤å¤–ï¼ŒGRPO åœ¨ä¸¤ä¸ªæ•°æ®é›†ä¸Šå‡æ˜¾è‘—ä¼˜äº PPOã€‚

## II. Proposed Solution

![](./images/solution.png)

The models will be fine-tuned to produce structured outputs with a reasoning section wrapped in `<THINK>` tags for step-by-step logic, followed by a precise medical answer in `<SOLUTION>` tags. We designed a two-stage pipeline here, **Supervised Fine-Tuning (SFT)** followed by **Reinforcement Learning (RL)**, to transform LLMs into structured medical reasoners:

-   **Phase 1 - SFT**: The goal here is not to teach the model to be a medical solver yet. It's to teach the model the grammar of our desired output. Here, we used a [dataset](https://huggingface.co/datasets/FreedomIntelligence/medical-o1-reasoning-SFT) of multiple medical problems with high-quality reasoning traces, formatted with our custom `<THINK>` and `<SOLUTION>` tags. This forces the model to learn the structural template we defined.
-   **Phase 2 - RL**: This is where we refine the logic using **RL**. Now that the model already knows how to structure its response, we then use [GRPO](https://arxiv.org/pdf/2402.03300) and this [medical questions dataset](https://huggingface.co/datasets/FreedomIntelligence/medical-o1-verifiable-problem) to teach it how to reason accurately and arrive at the correct medical answer.

[GRPO](https://arxiv.org/pdf/2402.03300) is an SOTA RL technique designed to overcome key limitations of the traditional [PPO](https://arxiv.org/abs/1707.06347). Specifically, [PPO](https://arxiv.org/abs/1707.06347) can suffer from high memory overhead due to its reliance on value network and instability in value function estimation. [GRPO](https://arxiv.org/pdf/2402.03300) addresses these issues by eliminating the need for a learned value function, instead using **group-relative advantage estimation** across multiple responses. This not only reduces computational cost but also improves training stability and scalability

Another drawback of [PPO](https://arxiv.org/abs/1707.06347) is that it also relies on a reward model that assigns **an absolute score to a generation**. There are 2 problems with this:

-   First, the reward model can rely on human judgments that usually lack explicit criteria and require expensive human annotation.
-   Second, it can be unstable as the LLM might learn to **hack** the reward. For example, it can generate very long completion if the length is correlated with a higher score. The solution here is to define a list of smaller verifiable rewards, not a final all consuming singular one.

With [GRPO](https://arxiv.org/pdf/2402.03300), we already generated **a group of responses** for each prompt right? Instead of scoring each one in isolation, we evaluate them relative to each other with our [multi-reward system](#III-multi-reward-system):

-   This **Reinforcement Learning with Verifiable Rewards** will allow us to further eliminate the need for a reward model and replace subjective human evaluation with reliable, objective signals.
-   This relative comparison is far more stable and directly optimizes for what we want: better reasoning, not just a higher score.

è¿™äº›æ¨¡å‹å°†é€šè¿‡å¾®è°ƒæ¥ç”Ÿæˆç»“æ„åŒ–è¾“å‡ºï¼šå…ˆåœ¨ <THINK> æ ‡ç­¾ä¸­ç»™å‡ºé€æ­¥æ¨ç†è¿‡ç¨‹ï¼Œå†åœ¨ <SOLUTION> æ ‡ç­¾ä¸­ç»™å‡ºç²¾ç¡®çš„åŒ»å­¦ç­”æ¡ˆã€‚
æˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªä¸¤é˜¶æ®µè®­ç»ƒæµæ°´çº¿â€”â€”å…ˆè¿›è¡Œç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ï¼Œå†è¿›è¡Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ï¼Œä»¥å°†é€šç”¨å¤§è¯­è¨€æ¨¡å‹è½¬åŒ–ä¸ºå…·å¤‡ç»“æ„åŒ–åŒ»å­¦æ¨ç†èƒ½åŠ›çš„æ¨¡å‹ã€‚

é˜¶æ®µä¸€ï¼šç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰

è¿™ä¸€é˜¶æ®µçš„ç›®æ ‡å¹¶ä¸æ˜¯æ•™æ¨¡å‹å¦‚ä½•çœŸæ­£è§£å†³åŒ»å­¦é—®é¢˜ï¼Œè€Œæ˜¯æ•™ä¼šæ¨¡å‹æˆ‘ä»¬æœŸæœ›çš„è¾“å‡ºè¯­æ³•å’Œç»“æ„ã€‚
åœ¨è¿™ä¸€é˜¶æ®µï¼Œæˆ‘ä»¬ä½¿ç”¨äº†ä¸€ä¸ªåŒ…å«å¤šç§åŒ»å­¦é—®é¢˜çš„æ•°æ®é›†ï¼Œå…¶ä¸­é…æœ‰é«˜è´¨é‡çš„æ¨ç†è½¨è¿¹ï¼Œå¹¶ç»Ÿä¸€é‡‡ç”¨è‡ªå®šä¹‰çš„ <THINK> å’Œ <SOLUTION> æ ‡ç­¾è¿›è¡Œæ ¼å¼åŒ–ã€‚
è¿™æ ·å¯ä»¥å¼ºåˆ¶æ¨¡å‹å­¦ä¹ å¹¶å†…åŒ–æˆ‘ä»¬æ‰€å®šä¹‰çš„è¾“å‡ºæ¨¡æ¿å’Œç»“æ„è§„èŒƒã€‚

é˜¶æ®µäºŒï¼šå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰

åœ¨æ¨¡å‹å·²ç»æŒæ¡è¾“å‡ºç»“æ„ä¹‹åï¼Œç¬¬äºŒé˜¶æ®µé€šè¿‡å¼ºåŒ–å­¦ä¹ æ¥æå‡å…¶æ¨ç†è´¨é‡å’Œç­”æ¡ˆæ­£ç¡®æ€§ã€‚
æˆ‘ä»¬ä½¿ç”¨ GRPOï¼ˆGroup Relative Policy Optimizationï¼‰ ä»¥åŠåŒ»å­¦é—®ç­”æ•°æ®é›†ï¼Œå¯¹æ¨¡å‹è¿›è¡Œè¿›ä¸€æ­¥è®­ç»ƒï¼Œä½¿å…¶èƒ½å¤Ÿæ›´å‡†ç¡®åœ°è¿›è¡Œæ¨ç†å¹¶å¾—åˆ°æ­£ç¡®çš„åŒ»å­¦ç»“è®ºã€‚

GRPO æ˜¯ä¸€ç§å…ˆè¿›çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œæ—¨åœ¨å…‹æœä¼ ç»Ÿ PPO çš„å…³é”®å±€é™æ€§ã€‚
å…·ä½“æ¥è¯´ï¼ŒPPO ç”±äºä¾èµ–ä»·å€¼ç½‘ç»œï¼ˆvalue networkï¼‰ï¼Œé€šå¸¸å­˜åœ¨ä»¥ä¸‹é—®é¢˜ï¼š
    1.æ˜¾å­˜å’Œè®¡ç®—å¼€é”€è¾ƒå¤§
    2.ä»·å€¼å‡½æ•°ä¼°è®¡ä¸ç¨³å®šï¼Œå®¹æ˜“å½±å“è®­ç»ƒç¨³å®šæ€§
        GRPO é€šè¿‡å®Œå…¨ç§»é™¤å¯¹ä»·å€¼å‡½æ•°çš„ä¾èµ–ï¼Œæ”¹ä¸ºåœ¨åŒä¸€é—®é¢˜ä¸‹ç”Ÿæˆçš„å¤šæ¡å›ç­”ä¹‹é—´è¿›è¡Œç›¸å¯¹ä¼˜åŠ¿ä¼°è®¡ï¼Œæœ‰æ•ˆè§£å†³äº†ä¸Šè¿°é—®é¢˜ã€‚è¿™ä¸ä»…é™ä½äº†è®¡ç®—æˆæœ¬ï¼Œè¿˜æ˜¾è‘—æå‡äº†è®­ç»ƒçš„ç¨³å®šæ€§å’Œå¯æ‰©å±•æ€§ã€‚
    3.PPO çš„å¦ä¸€é¡¹ç¼ºé™·ä¸ GRPO çš„æ”¹è¿›
        PPO è¿˜ä¾èµ–äºä¸€ä¸ªå¥–åŠ±æ¨¡å‹ï¼ˆReward Modelï¼‰ï¼Œä¸ºæ¯æ¡ç”Ÿæˆç»“æœæ‰“ä¸€ä¸ªç»å¯¹åˆ†æ•°ï¼Œè€Œè¿™æœ¬èº«å­˜åœ¨ä¸¤ä¸ªä¸¥é‡é—®é¢˜ï¼š
        å¥–åŠ±æ¨¡å‹å¾€å¾€ä¾èµ–äººå·¥æ ‡æ³¨ï¼Œè€Œäººç±»è¯„ä¼°é€šå¸¸ç¼ºä¹æ˜ç¡®ã€å¯æ‰§è¡Œçš„è¯„ä»·æ ‡å‡†ï¼Œä¸”æ ‡æ³¨æˆæœ¬æé«˜ã€‚
        å¥–åŠ±ä¸ç¨³å®šï¼Œå®¹æ˜“è¢«æ¨¡å‹â€œæŠ•æœºå–å·§â€ã€‚ä¾‹å¦‚ï¼Œå¦‚æœå¥–åŠ±ä¸ç”Ÿæˆé•¿åº¦ç›¸å…³ï¼Œæ¨¡å‹å¯èƒ½ä¼šå€¾å‘äºè¾“å‡ºå†—é•¿ä½†ä½è´¨é‡çš„å†…å®¹ã€‚
        è§£å†³æ€è·¯æ˜¯ï¼š
            ä¸ä½¿ç”¨å•ä¸€ã€ç¬¼ç»Ÿçš„å¥–åŠ±ä¿¡å·ï¼Œè€Œæ˜¯è®¾è®¡å¤šä¸ªå¯éªŒè¯çš„ã€ç»†ç²’åº¦çš„å¥–åŠ±å‡½æ•°ã€‚
            åŸºäº GRPO çš„ç›¸å¯¹å¥–åŠ±ä¸å¯éªŒè¯å¥–åŠ±
            åœ¨ GRPO ä¸­ï¼Œæˆ‘ä»¬ä¼šä¸ºåŒä¸€ä¸ªé—®é¢˜ç”Ÿæˆä¸€ç»„å€™é€‰å›ç­”ã€‚
            ä¸å…¶å¯¹æ¯æ¡å›ç­”è¿›è¡Œå­¤ç«‹è¯„åˆ†ï¼Œä¸å¦‚åœ¨åŒç»„å›ç­”ä¹‹é—´è¿›è¡Œç›¸å¯¹æ¯”è¾ƒï¼Œå¹¶ç»“åˆæˆ‘ä»¬è®¾è®¡çš„å¤šé‡å¯éªŒè¯å¥–åŠ±æœºåˆ¶è¿›è¡Œè¯„ä¼°ã€‚
            è¿™ç§åŸºäºå¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ï¼ˆReinforcement Learning with Verifiable Rewardsï¼‰ï¼Œå¯ä»¥è¿›ä¸€æ­¥æ¶ˆé™¤å¯¹å¥–åŠ±æ¨¡å‹çš„ä¾èµ–ï¼Œç”¨å®¢è§‚ã€ç¨³å®šã€å¯è‡ªåŠ¨éªŒè¯çš„ä¿¡å·å–ä»£ä¸»è§‚çš„äººç±»è¯„ä¼°ã€‚
            è¿™ç§ç›¸å¯¹æ¯”è¾ƒæ–¹å¼æ›´åŠ ç¨³å®šï¼Œä¹Ÿæ›´ç›´æ¥åœ°ä¼˜åŒ–äº†æˆ‘ä»¬çœŸæ­£å…³å¿ƒçš„ç›®æ ‡ï¼š
                ğŸ‘‰ æ›´å¥½çš„æ¨ç†èƒ½åŠ›ï¼Œè€Œä¸ä»…ä»…æ˜¯æ›´é«˜çš„å¥–åŠ±åˆ†æ•°ã€‚


## III. Multi-Reward System


![](./images/rewards.png)

Our core innovation is this multi-reward design. A single reward is not enough to capture the nuances of good medical reasoning. We designed a **panel of 4 expert judges** working in parallel, each evaluating the model's output from a different perspective:

1.  The first is the **Strict Formatter** which strictly evaluate format compliance to enforce the structure. It gives a large reward only if the entire response perfectly adheres to our `THINK` and `ANSWER` structure.

2.  The second is the **Partial Formatter** giving partial credit for incomplete tags. If the model messes up the full structure, but for example, still includes the `</THINK>` tag correctly, it still gets a small amount of credit.

3.  The third, also the **most important one**. It will check if the answer in the `<ANSWER>` tag is correct or not. Given the prevalence of aliases in the medical domain, exact matching methods, which commonly applied in mathematics, will be impractical here. Instead, as suggested by [HuatuoGPT-o1](https://arxiv.org/pdf/2412.18925), we use an [LLM verifier](https://huggingface.co/FreedomIntelligence/medical_o1_verifier_3B) here and prompt it to perform validation, returning a probability of how close the prediction aligns with the ground-truth answer. We designed this function to be sophisticated, giving full marks for an high probability, partial credit for close approximations, and a heavy penalty for wrong answers to avoid overconfidence.

By combining these 3 signals, we can prevent over-optimization on 1 aspect, which can lead to reward hacking problem. The [GRPO](https://arxiv.org/pdf/2402.03300)'s group-relative policy can navigate the complex trade-offs between formatting, correctness, readability, and optimizes by ranking completions, leading to a much more capable and reliable reasoning model.

æˆ‘ä»¬çš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå¤šå¥–åŠ±ï¼ˆmulti-rewardï¼‰è®¾è®¡ã€‚å•ä¸€å¥–åŠ±ä¿¡å·ä¸è¶³ä»¥åˆ»ç”»é«˜è´¨é‡åŒ»å­¦æ¨ç†ä¸­è•´å«çš„å¤æ‚ç»†èŠ‚ã€‚å› æ­¤ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç»„å¹¶è¡Œå·¥ä½œçš„å››ä½â€œä¸“å®¶è¯„å®¡â€ï¼Œä»ä¸åŒç»´åº¦å¯¹æ¨¡å‹è¾“å‡ºè¿›è¡Œè¯„ä¼°ï¼š
ç¬¬ä¸€ä½è¯„å®¡æ˜¯ä¸¥æ ¼æ ¼å¼è¯„å®¡ï¼ˆStrict Formatterï¼‰ï¼Œç”¨äºä¸¥æ ¼æ£€æŸ¥è¾“å‡ºæ˜¯å¦å®Œå…¨ç¬¦åˆæˆ‘ä»¬é¢„å®šä¹‰çš„ç»“æ„è§„èŒƒã€‚åªæœ‰å½“æ¨¡å‹çš„å®Œæ•´å›ç­”ä¸¥æ ¼éµå¾ª <THINK> ä¸ <ANSWER> çš„ç»“æ„è¦æ±‚æ—¶ï¼Œæ‰ä¼šç»™äºˆè¾ƒé«˜å¥–åŠ±ã€‚
ç¬¬äºŒä½è¯„å®¡æ˜¯éƒ¨åˆ†æ ¼å¼è¯„å®¡ï¼ˆPartial Formatterï¼‰ï¼Œç”¨äºå¯¹ä¸å®Œæ•´ä½†éƒ¨åˆ†åˆè§„çš„ç»“æ„ç»™äºˆâ€œéƒ¨åˆ†å¥–åŠ±â€ã€‚ä¾‹å¦‚ï¼Œå½“æ¨¡å‹æœªèƒ½å®Œå…¨éµå®ˆç»“æ„è§„èŒƒï¼Œä½†ä»æ­£ç¡®åŒ…å«äº† </THINK> ç­‰å…³é”®æ ‡ç­¾æ—¶ï¼Œä»å¯è·å¾—ä¸€å®šçš„æ­£å‘åé¦ˆã€‚
ç¬¬ä¸‰ä½è¯„å®¡ä¹Ÿæ˜¯æœ€é‡è¦çš„ä¸€ä½ï¼Œè´Ÿè´£è¯„ä¼°ç­”æ¡ˆçš„æ­£ç¡®æ€§ã€‚
åœ¨åŒ»å­¦é¢†åŸŸä¸­ï¼Œç”±äºåŒä¹‰è¯ã€åˆ«åå’Œè¡¨è¾¾å¤šæ ·æ€§å¹¿æ³›å­˜åœ¨ï¼Œæ•°å­¦é¢†åŸŸå¸¸ç”¨çš„ç²¾ç¡®åŒ¹é…æ–¹æ³•å¹¶ä¸é€‚ç”¨ã€‚å— HuatuoGPT-o1 çš„å¯å‘ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ä¸€ä¸ª LLM éªŒè¯å™¨ï¼ˆLLM-based verifierï¼‰ æ¥å®Œæˆè¿™ä¸€ä»»åŠ¡ã€‚è¯¥éªŒè¯å™¨ä¼šè¯„ä¼°æ¨¡å‹åœ¨ <ANSWER> æ ‡ç­¾ä¸­çš„é¢„æµ‹ç»“æœä¸æ ‡å‡†ç­”æ¡ˆä¹‹é—´çš„ä¸€è‡´ç¨‹åº¦ï¼Œå¹¶è¾“å‡ºä¸€ä¸ªæ¦‚ç‡å€¼ã€‚
è¯¥å¥–åŠ±å‡½æ•°è¢«ç²¾å¿ƒè®¾è®¡ï¼šå½“ä¸€è‡´æ€§æ¦‚ç‡è¾ƒé«˜æ—¶ç»™äºˆæ»¡åˆ†å¥–åŠ±ï¼Œå¯¹æ¥è¿‘æ­£ç¡®çš„é¢„æµ‹ç»™äºˆéƒ¨åˆ†å¥–åŠ±ï¼Œè€Œå¯¹äºæ˜æ˜¾é”™è¯¯çš„ç­”æ¡ˆæ–½åŠ æ˜¾è‘—æƒ©ç½šï¼Œä»¥é¿å…æ¨¡å‹äº§ç”Ÿè¿‡åº¦è‡ªä¿¡çš„é”™è¯¯åˆ¤æ–­ã€‚
é€šè¿‡èåˆè¿™ä¸‰ç±»å¥–åŠ±ä¿¡å·ï¼Œæˆ‘ä»¬èƒ½å¤Ÿæœ‰æ•ˆé¿å…æ¨¡å‹åœ¨å•ä¸€ç»´åº¦ä¸Šè¿‡åº¦ä¼˜åŒ–ï¼Œä»è€Œå‡å°‘å¥–åŠ±æŠ•æœºï¼ˆreward hackingï¼‰é—®é¢˜çš„å‘ç”Ÿã€‚
å€ŸåŠ© GRPO çš„ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–æœºåˆ¶ï¼ˆgroup-relative policyï¼‰ï¼Œæ¨¡å‹å¯ä»¥åœ¨æ ¼å¼è§„èŒƒæ€§ã€ç­”æ¡ˆæ­£ç¡®æ€§ã€å¯è¯»æ€§ç­‰å¤šç§ç›®æ ‡ä¹‹é—´è¿›è¡Œæƒè¡¡ï¼Œå¹¶é€šè¿‡å¯¹å¤šä¸ªå€™é€‰ç”Ÿæˆç»“æœè¿›è¡Œæ’åºæ¥å®Œæˆä¼˜åŒ–ï¼Œæœ€ç»ˆå¾—åˆ°ä¸€ä¸ªæ›´å¼ºå¤§ã€æ›´ç¨³å®šä¸”æ›´å¯ä¿¡çš„åŒ»å­¦æ¨ç†æ¨¡å‹ã€‚

## IV. GRPO Objective Improvement

![](./images/grpo.png)

Compared to the original formulation in the [DeepSeekMath](https://arxiv.org/pdf/2402.03300) paper, we followed [Hugging Face's GRPO guideline](https://huggingface.co/docs/trl/main/en/grpo_trainer#computing-the-loss) and made some further improvements to the [GRPO](https://arxiv.org/pdf/2402.03300) objective for more efficient training:

-   First, we can calculate the _mean_ at the _group_ and the _std_ at the _batch_ level. This scaling strategy enables more robust reward shaping, as evident by this [paper](https://huggingface.co/papers/2508.08221).
-   Second, we didn't use the **KL divergence** term, as motivated by several recent studies, which showed that **KL** term is not essential for training with [GRPO](https://arxiv.org/pdf/2402.03300). Therefore, it has become a common practice to exclude it.
-   Lastly, this [paper](https://huggingface.co/papers/2503.20783) has demonstrated that the initial [GRPO](https://arxiv.org/pdf/2402.03300) formulation introduces a **response length bias**. To solve that, they proposed dividing by a **constant generation budget** instead of the sequence length, so we employ this [Dr.GRPO](https://huggingface.co/papers/2503.20783) loss here to further enhances stability by preventing the model from being biased towards longer or shorter answers, focusing purely on the quality of the content.

ç›¸æ¯” DeepSeekMath è®ºæ–‡ä¸­çš„åŸå§‹å…¬å¼ï¼Œæˆ‘ä»¬éµå¾ªäº† Hugging Face çš„ GRPO æŒ‡å—ï¼Œå¹¶å¯¹ GRPO ç›®æ ‡å‡½æ•°åšäº†ä¸€äº›è¿›ä¸€æ­¥çš„æ”¹è¿›ï¼Œä»¥æå‡è®­ç»ƒæ•ˆç‡ï¼š
é¦–å…ˆï¼Œæˆ‘ä»¬åœ¨ group çº§åˆ« è®¡ç®—å‡å€¼ï¼Œåœ¨ batch çº§åˆ« è®¡ç®—æ ‡å‡†å·®ã€‚è¿™ç§ç¼©æ”¾ç­–ç•¥èƒ½å¤Ÿå¸¦æ¥æ›´ç¨³å¥çš„å¥–åŠ±å¡‘å½¢æ•ˆæœï¼Œç›¸å…³ç»“è®ºä¹Ÿå·²åœ¨è¯¥è®ºæ–‡ä¸­å¾—åˆ°éªŒè¯ã€‚
å…¶æ¬¡ï¼Œæˆ‘ä»¬æ²¡æœ‰ä½¿ç”¨ KL æ•£åº¦é¡¹ã€‚è¿™ä¸€åšæ³•å—åˆ°å¤šé¡¹è¿‘æœŸç ”ç©¶çš„å¯å‘ï¼Œè¿™äº›ç ”ç©¶è¡¨æ˜ï¼Œåœ¨ GRPO è®­ç»ƒä¸­ KL é¡¹å¹¶éå¿…éœ€ï¼Œå› æ­¤åœ¨å®è·µä¸­å°†å…¶ç§»é™¤å·²æˆä¸ºä¸€ç§å¸¸è§åšæ³•ã€‚
æœ€åï¼Œæœ‰ç ”ç©¶æŒ‡å‡ºï¼Œæœ€åˆçš„ GRPO å…¬å¼ä¼šå¼•å…¥å¯¹ç”Ÿæˆé•¿åº¦çš„åç½®ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œè®ºæ–‡æå‡ºç”¨ä¸€ä¸ªå›ºå®šçš„ç”Ÿæˆé¢„ç®—è€Œä¸æ˜¯åºåˆ—é•¿åº¦è¿›è¡Œå½’ä¸€åŒ–ã€‚å› æ­¤ï¼Œæˆ‘ä»¬åœ¨è¿™é‡Œé‡‡ç”¨äº† Dr.GRPO lossï¼Œé€šè¿‡é¿å…æ¨¡å‹åå‘ç”Ÿæˆæ›´é•¿æˆ–æ›´çŸ­çš„å›ç­”ï¼Œè¿›ä¸€æ­¥æå‡äº†è®­ç»ƒç¨³å®šæ€§ï¼Œä½¿æ¨¡å‹ä¸“æ³¨äºå†…å®¹è´¨é‡æœ¬èº«ã€‚

## V. Experimental Results

![](./images/results.png)

ğŸ‘‰ You can refer to our [slides](./slides.pdf) and [full report](./report.pdf) for more details on the methodology and results analysis.

## VI. References

**1. [HuatuoGPT-o1](https://github.com/FreedomIntelligence/HuatuoGPT-o1)**:

-   We adapted many ideas from their work on medical reasoning with LLMs.
-   We used their [PPO](https://arxiv.org/abs/1707.06347) approach as a baseline to compare against our [GRPO](https://arxiv.org/pdf/2402.03300) solution. Note that, we used smaller model here due to computational constraints on Colab Pro Environment.

**2. Hugging Face's Cookbook**:

-   [GRPO Trainer Documentation](https://huggingface.co/docs/trl/main/en/grpo_trainer#computing-the-loss).
-   [Post training an LLM for reasoning with GRPO in TRL](https://huggingface.co/learn/cookbook/fine_tuning_llm_grpo_trl).
-   [HuatuoGPT-o1 Medical RAG and Reasoning](https://huggingface.co/learn/cookbook/medical_rag_and_reasoning): We followed this to build our demo with RAG capabilities.

**3. Unsloth Documentation**:

> We mainly used [Unsloth](https://docs.unsloth.ai/) to implement our [GRPO](https://arxiv.org/pdf/2402.03300) training.

-   [Reinforcement Learning (RL) Guide](https://docs.unsloth.ai/get-started/reinforcement-learning-rl-guide).

-   [GRPO (Reasoning RL) notebooks](https://docs.unsloth.ai/get-started/unsloth-notebooks#grpo-reasoning-rl-notebooks): We learned a lot from these notebooks.
