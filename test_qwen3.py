#!/usr/bin/env python3
"""
Qwen3-0.6B æ¨¡å‹åŠ è½½ä¸æµ‹è¯•ç¨‹åº
æ”¯æŒæœ¬åœ°æ¨¡å‹åŠ è½½å’ŒåŸºæœ¬æ¨ç†æµ‹è¯•
"""

import torch
import json
import time
from transformers import AutoModelForCausalLM, AutoTokenizer
from typing import Dict, Any, List

class Qwen3Loader:
    """Qwen3æ¨¡å‹åŠ è½½å™¨"""
    
    def __init__(self, model_path: str):
        """
        åˆå§‹åŒ–Qwen3æ¨¡å‹åŠ è½½å™¨
        
        Args:
            model_path: æœ¬åœ°æ¨¡å‹è·¯å¾„
        """
        self.model_path = model_path
        self.model = None
        self.tokenizer = None
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
    def load_model(self):
        """åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨"""
        print(f"æ­£åœ¨ä» {self.model_path} åŠ è½½æ¨¡å‹...")
        print(f"ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        try:
            # åŠ è½½tokenizer
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.model_path,
                trust_remote_code=True,
                use_fast=False
            )
            
            # åŠ è½½æ¨¡å‹
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_path,
                torch_dtype=torch.bfloat16 if self.device.type == "cuda" else torch.float32,
                device_map="auto" if self.device.type == "cuda" else None,
                trust_remote_code=True,
                use_cache=True
            )
            
            if self.device.type == "cpu":
                self.model = self.model.to(self.device)
            
            print("âœ… æ¨¡å‹å’Œåˆ†è¯å™¨åŠ è½½æˆåŠŸ!")
            self.print_model_info()
            
        except Exception as e:
            print(f"âŒ åŠ è½½æ¨¡å‹å¤±è´¥: {e}")
            raise
    
    def print_model_info(self):
        """æ‰“å°æ¨¡å‹ä¿¡æ¯"""
        if self.model is None:
            return
            
        print("\n" + "="*50)
        print("ğŸ“Š æ¨¡å‹ä¿¡æ¯:")
        print(f"æ¨¡å‹ç±»å‹: {type(self.model).__name__}")
        print(f"å‚æ•°é‡: {sum(p.numel() for p in self.model.parameters()):,}")
        print(f"å¯è®­ç»ƒå‚æ•°é‡: {sum(p.numel() for p in self.model.parameters() if p.requires_grad):,}")
        print(f"è¯æ±‡è¡¨å¤§å°: {self.tokenizer.vocab_size:,}")
        print(f"æœ€å¤§åºåˆ—é•¿åº¦: {self.tokenizer.model_max_length}")
        print("="*50)
    
    def generate_text(self, prompt: str, max_length: int = 512, temperature: float = 0.7, 
                     top_p: float = 0.9, do_sample: bool = True) -> str:
        """
        ç”Ÿæˆæ–‡æœ¬
        
        Args:
            prompt: è¾“å…¥æç¤º
            max_length: æœ€å¤§ç”Ÿæˆé•¿åº¦
            temperature: æ¸©åº¦å‚æ•°
            top_p: top_pé‡‡æ ·å‚æ•°
            do_sample: æ˜¯å¦é‡‡æ ·
            
        Returns:
            ç”Ÿæˆçš„æ–‡æœ¬
        """
        if self.model is None or self.tokenizer is None:
            raise RuntimeError("è¯·å…ˆåŠ è½½æ¨¡å‹!")
        
        # å¯¹è¾“å…¥è¿›è¡Œç¼–ç 
        inputs = self.tokenizer.encode(prompt, return_tensors="pt").to(self.device)
        
        # ç”Ÿæˆæ–‡æœ¬
        start_time = time.time()
        
        with torch.no_grad():
            outputs = self.model.generate(
                inputs,
                max_length=max_length,
                temperature=temperature,
                top_p=top_p,
                do_sample=do_sample,
                pad_token_id=self.tokenizer.eos_token_id,
                eos_token_id=self.tokenizer.eos_token_id,
                repetition_penalty=1.1
            )
        
        generation_time = time.time() - start_time
        
        # è§£ç è¾“å‡º
        generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        # åªè¿”å›ç”Ÿæˆçš„éƒ¨åˆ†ï¼ˆå»æ‰è¾“å…¥æç¤ºï¼‰
        if generated_text.startswith(prompt):
            result = generated_text[len(prompt):].strip()
        else:
            result = generated_text.strip()
        
        print(f"â±ï¸ ç”Ÿæˆè€—æ—¶: {generation_time:.2f}ç§’")
        
        return result
    
    def chat(self, user_input: str, system_prompt: str = "ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„AIåŠ©æ‰‹ã€‚") -> str:
        """
        èŠå¤©å¯¹è¯
        
        Args:
            user_input: ç”¨æˆ·è¾“å…¥
            system_prompt: ç³»ç»Ÿæç¤º
            
        Returns:
            AIå›å¤
        """
        # æ„å»ºå¯¹è¯æ ¼å¼
        prompt = f"<|im_start|>system\n{system_prompt}<|im_end|>\n<|im_start|>user\n{user_input}<|im_end|>\n<|im_start|>assistant\n"
        
        return self.generate_text(prompt)


def run_tests():
    """è¿è¡Œæ¨¡å‹æµ‹è¯•"""
    print("ğŸš€ å¼€å§‹Qwen3-0.6Bæ¨¡å‹æµ‹è¯•")
    
    # æ¨¡å‹è·¯å¾„
    model_path = "output/sft_merge"
    
    try:
        # åˆå§‹åŒ–åŠ è½½å™¨
        loader = Qwen3Loader(model_path)
        
        # åŠ è½½æ¨¡å‹
        loader.load_model()
        
        # æµ‹è¯•ç”¨ä¾‹
        test_cases = [
            {
                "name": "åŸºç¡€é—®ç­”æµ‹è¯•",
                "input": "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ã€‚",
                "type": "chat"
            },
            {
                "name": "çŸ¥è¯†é—®ç­”æµ‹è¯•", 
                "input": "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ",
                "type": "chat"
            },
            {
                "name": "ä»£ç ç”Ÿæˆæµ‹è¯•",
                "input": "è¯·ç”¨Pythonå†™ä¸€ä¸ªè®¡ç®—æ–æ³¢é‚£å¥‘æ•°åˆ—çš„å‡½æ•°ã€‚",
                "type": "chat"
            },
            {
                "name": "åˆ›æ„å†™ä½œæµ‹è¯•",
                "input": "è¯·å†™ä¸€é¦–å…³äºæ˜¥å¤©çš„çŸ­è¯—ã€‚",
                "type": "chat"
            },
            {
                "name": "æ–‡æœ¬è¡¥å…¨æµ‹è¯•",
                "input": "äººå·¥æ™ºèƒ½çš„æœªæ¥å‘å±•è¶‹åŠ¿æ˜¯",
                "type": "generate",
                "params": {"max_length": 200, "temperature": 0.8}
            }
        ]
        
        # è¿è¡Œæµ‹è¯•
        print("\n" + "ğŸ§ª"*20)
        print("å¼€å§‹æ‰§è¡Œæµ‹è¯•ç”¨ä¾‹...")
        print("ğŸ§ª"*20)
        
        for i, test_case in enumerate(test_cases, 1):
            print(f"\n{'='*60}")
            print(f"æµ‹è¯• {i}/{len(test_cases)}: {test_case['name']}")
            print(f"è¾“å…¥: {test_case['input']}")
            print("-"*60)
            
            if test_case['type'] == 'chat':
                response = loader.chat(test_case['input'])
            else:  # generate
                params = test_case.get('params', {})
                response = loader.generate_text(test_case['input'], **params)
            
            print(f"è¾“å‡º: {response}")
            print(f"{'='*60}")
        
        # æ€§èƒ½æµ‹è¯•
        print("\n" + "âš¡"*20)
        print("æ€§èƒ½æµ‹è¯•...")
        print("âš¡"*20)
        
        test_prompt = "è¯·ç®€å•ä»‹ç»ä¸€ä¸‹æ·±åº¦å­¦ä¹ çš„åŸºæœ¬æ¦‚å¿µã€‚"
        
        # æµ‹è¯•ä¸åŒå‚æ•°
        configs = [
            {"temperature": 0.1, "name": "ä½æ¸©åº¦é‡‡æ ·"},
            {"temperature": 0.7, "name": "ä¸­ç­‰æ¸©åº¦é‡‡æ ·"},
            {"temperature": 1.0, "name": "é«˜æ¸©åº¦é‡‡æ ·"}
        ]
        
        for config in configs:
            print(f"\næµ‹è¯•é…ç½®: {config['name']} (temperature={config['temperature']})")
            start_time = time.time()
            response = loader.chat(test_prompt)
            end_time = time.time()
            print(f"å›å¤: {response}")
            print(f"æ€»è€—æ—¶: {end_time - start_time:.2f}ç§’")
        
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•å®Œæˆ!")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


def interactive_mode():
    """äº¤äº’å¼å¯¹è¯æ¨¡å¼"""
    print("ğŸ’¬ è¿›å…¥äº¤äº’å¼å¯¹è¯æ¨¡å¼")
    print("è¾“å…¥ 'quit' æˆ– 'exit' é€€å‡º")
    
    model_path = "model/sft"
    loader = Qwen3Loader(model_path)
    loader.load_model()
    
    while True:
        try:
            user_input = input("\nç”¨æˆ·: ").strip()
            
            if user_input.lower() in ['quit', 'exit', 'é€€å‡º']:
                print("ğŸ‘‹ å†è§!")
                break
            
            if not user_input:
                continue
            
            print("AI: ", end="", flush=True)
            response = loader.chat(user_input)
            print(response)
            
        except KeyboardInterrupt:
            print("\nğŸ‘‹ å†è§!")
            break
        except Exception as e:
            print(f"âŒ ç”Ÿæˆå›å¤æ—¶å‡ºé”™: {e}")


if __name__ == "__main__":
    import sys
    
    if len(sys.argv) > 1 and sys.argv[1] == "interactive":
        interactive_mode()
    else:
        run_tests()