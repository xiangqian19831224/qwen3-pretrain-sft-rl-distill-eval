# Qwen3-32B VLLM å¤šæœºå¤šå¡åˆ†å¸ƒå¼éƒ¨ç½²

æœ¬ç›®å½•åŒ…å«äº†åŸºäºVLLMæ¡†æ¶éƒ¨ç½²Qwen3-32Bæ¨¡å‹çš„å¤šæœºå¤šå¡åˆ†å¸ƒå¼æ–¹æ¡ˆã€‚

## ğŸ—ï¸ æ¶æ„æ¦‚è¿°

- **æ¨¡å‹**: Qwen3-32B (32Bå‚æ•°)
- **æ¡†æ¶**: VLLM + Ray
- **å¹¶è¡Œç­–ç•¥**: Tensor Parallel + Pipeline Parallel
- **éƒ¨ç½²æ–¹å¼**: å¤šæœºå¤šå¡åˆ†å¸ƒå¼æ¨ç†

## ğŸ“‹ ç¯å¢ƒè¦æ±‚

### ç¡¬ä»¶è¦æ±‚
- **æœ€å°é…ç½®**: 2å°æœåŠ¡å™¨ï¼Œæ¯å°è‡³å°‘2å¼ GPU (å»ºè®®A100 40GBæˆ–H100 80GB)
- **æ¨èé…ç½®**: 4å°æœåŠ¡å™¨ï¼Œæ¯å°2-4å¼ GPU
- **ç½‘ç»œ**: ä¸‡å…†ä»¥å¤ªç½‘æˆ–InfiniBand
- **å­˜å‚¨**: å…±äº«å­˜å‚¨æˆ–æ¯ä¸ªèŠ‚ç‚¹æœ¬åœ°å­˜å‚¨æ¨¡å‹æ–‡ä»¶

### è½¯ä»¶è¦æ±‚
- CUDA >= 12.1
- Python >= 3.9
- Docker (å¯é€‰)
- Kubernetes (å¯é€‰)

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. ç¯å¢ƒå‡†å¤‡

```bash
# å®‰è£…ä¾èµ–
./deploy_commands.sh install

# æ£€æŸ¥ç¯å¢ƒ
./deploy_commands.sh check
```

### 2. é…ç½®é›†ç¾¤

ç¼–è¾‘ `cluster_config.json` æ–‡ä»¶ï¼Œé…ç½®ä½ çš„é›†ç¾¤ä¿¡æ¯ï¼š

```json
{
    "model_path": "../../model/sft_merge",
    "world_size": 4,
    "tensor_parallel_size": 2,
    "pipeline_parallel_size": 2,
    "master": {
        "ip": "192.168.1.100",
        "port": 29500,
        "user": "username",
        "work_dir": "/path/to/deployment"
    },
    "workers": [
        {
            "ip": "192.168.1.101",
            "user": "username", 
            "work_dir": "/path/to/deployment"
        },
        {
            "ip": "192.168.1.102",
            "user": "username",
            "work_dir": "/path/to/deployment"
        }
    ]
}
```

### 3. éƒ¨ç½²æ–¹å¼

#### æ–¹å¼1: Pythonè„šæœ¬éƒ¨ç½² (æ¨è)

```bash
# å¯åŠ¨æ•´ä¸ªé›†ç¾¤
python launch_cluster.py --config cluster_config.json --mode all

# ä»…å¯åŠ¨MasterèŠ‚ç‚¹
python launch_cluster.py --config cluster_config.json --mode master

# ä»…å¯åŠ¨WorkerèŠ‚ç‚¹
python launch_cluster.py --config cluster_config.json --mode worker
```

#### æ–¹å¼2: æ‰‹åŠ¨éƒ¨ç½²

```bash
# MasterèŠ‚ç‚¹
bash vllm_distributed_master.sh

# WorkerèŠ‚ç‚¹1
WORKER_RANK=1 bash vllm_distributed_worker.sh

# WorkerèŠ‚ç‚¹2  
WORKER_RANK=2 bash vllm_distributed_worker.sh
```

#### æ–¹å¼3: Dockeréƒ¨ç½²

```bash
# æ„å»ºé•œåƒ
docker build -t vllm-qwen3:latest .

# å¯åŠ¨Master
docker run -d --gpus all -p 8000:8000 -p 8265:8265 \
  -v $(pwd)/model:/app/model \
  -e MASTER_IP=192.168.1.100 \
  --name vllm-master vllm-qwen3:latest

# å¯åŠ¨Worker
docker run -d --gpus all \
  -v $(pwd)/model:/app/model \
  -e MASTER_IP=192.168.1.100 \
  -e WORKER_RANK=1 \
  --name vllm-worker1 vllm-qwen3:latest
```

#### æ–¹å¼4: Kuberneteséƒ¨ç½²

```bash
kubectl apply -f k8s-deployment.yaml
```

## ğŸ“Š éƒ¨ç½²å‘½ä»¤é€ŸæŸ¥

```bash
# æŸ¥çœ‹æ‰€æœ‰éƒ¨ç½²é€‰é¡¹
./deploy_commands.sh

# ç¯å¢ƒæ£€æŸ¥
./deploy_commands.sh check

# ç›‘æ§é›†ç¾¤
./deploy_commands.sh monitor

# æ•…éšœæ’é™¤
./deploy_commands.sh troubleshoot

# æ€§èƒ½è°ƒä¼˜
./deploy_commands.sh tuning
```

## ğŸ§ª æµ‹è¯•éªŒè¯

éƒ¨ç½²å®Œæˆåï¼Œè¿è¡Œæµ‹è¯•è„šæœ¬éªŒè¯åŠŸèƒ½ï¼š

```bash
# å®Œæ•´æµ‹è¯•
python test_distributed.py --url http://192.168.1.100:8000

# å•ç‹¬æµ‹è¯•
python test_distributed.py --test health
python test_distributed.py --test chat
python test_distributed.py --test concurrent --concurrent-requests 20
python test_distributed.py --test streaming
```

## ğŸ“ˆ æ€§èƒ½è°ƒä¼˜

### å…³é”®å‚æ•°

| å‚æ•° | æ¨èå€¼ | è¯´æ˜ |
|------|--------|------|
| `--gpu-memory-utilization` | 0.85-0.95 | GPUå†…å­˜åˆ©ç”¨ç‡ |
| `--max-num-seqs` | 256-512 | æœ€å¤§å¹¶å‘åºåˆ—æ•° |
| `--max-model-len` | 8192-16384 | æœ€å¤§åºåˆ—é•¿åº¦ |
| `--block-size` | 16-32 | å—å¤§å° |
| `--tensor-parallel-size` | 2-4 | å¼ é‡å¹¶è¡Œåº¦ |
| `--pipeline-parallel-size` | 2-4 | æµæ°´å¹¶è¡Œåº¦ |

### ä¼˜åŒ–æŠ€å·§

1. **å¯ç”¨å‰ç¼€ç¼“å­˜**: `--enable-prefix-caching`
2. **ä½¿ç”¨é‡åŒ–**: `--quantization fp8` æˆ– `--quantization int4`
3. **æŠ•æœºè§£ç **: `--speculative-model path/to/draft_model`
4. **æ‰¹å¤„ç†ä¼˜åŒ–**: è°ƒæ•´ `--max-num-batched-tokens`

## ğŸ” ç›‘æ§æŒ‡æ ‡

### Ray Dashboard
- URL: `http://master-ip:8265`
- ç›‘æ§é›†ç¾¤çŠ¶æ€ã€èµ„æºä½¿ç”¨ã€ä»»åŠ¡æ‰§è¡Œæƒ…å†µ

### APIç›‘æ§
```bash
# å¥åº·æ£€æŸ¥
curl http://master-ip:8000/health

# æ¨¡å‹ä¿¡æ¯
curl http://master-ip:8000/v1/models

# æ€§èƒ½æŒ‡æ ‡
curl http://master-ip:8000/metrics
```

## ğŸ› ï¸ æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

1. **ç½‘ç»œè¿æ¥å¤±è´¥**
   ```bash
   ping worker-ip
   telnet worker-ip 29500
   ```

2. **GPUå†…å­˜ä¸è¶³**
   ```bash
   nvidia-smi
   # å‡å°‘ --max-num-seqs æˆ– --gpu-memory-utilization
   ```

3. **æ¨¡å‹åŠ è½½å¤±è´¥**
   ```bash
   # æ£€æŸ¥æ¨¡å‹è·¯å¾„
   ls -la ../../model/sft_merge
   # ç¡®ä¿æ‰€æœ‰èŠ‚ç‚¹éƒ½æœ‰æ¨¡å‹æ–‡ä»¶è®¿é—®æƒé™
   ```

4. **Rayé›†ç¾¤å¼‚å¸¸**
   ```bash
   ray stop
   ray start --head --port=6379
   ```

### æ—¥å¿—æŸ¥çœ‹

```bash
# VLLMæ—¥å¿—
tail -f ray_logs/worker*.out

# Rayæ—¥å¿—
tail -f ray_logs/raylet.out

# ç³»ç»Ÿæ—¥å¿—
journalctl -u vllm -f
```

## ğŸ“š APIä½¿ç”¨ç¤ºä¾‹

### OpenAIå…¼å®¹API

```bash
curl -X POST http://master-ip:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "Qwen/Qwen2.5-32B-Instruct",
    "messages": [{"role": "user", "content": "ä½ å¥½ï¼"}],
    "max_tokens": 100,
    "temperature": 0.7
  }'
```

### Pythonå®¢æˆ·ç«¯

```python
import openai

client = openai.OpenAI(
    base_url="http://master-ip:8000/v1",
    api_key="your-api-key"
)

response = client.chat.completions.create(
    model="Qwen/Qwen2.5-32B-Instruct",
    messages=[{"role": "user", "content": "ä½ å¥½ï¼"}],
    max_tokens=100
)

print(response.choices[0].message.content)
```

## ğŸ” å®‰å…¨é…ç½®

### APIè®¤è¯

```bash
# è®¾ç½®APIå¯†é’¥
export VLLM_API_KEY="your-secret-key"

# åœ¨è¯·æ±‚ä¸­ä½¿ç”¨
curl -H "Authorization: Bearer your-secret-key" \
     http://master-ip:8000/v1/models
```

### ç½‘ç»œå®‰å…¨

- ä½¿ç”¨é˜²ç«å¢™é™åˆ¶è®¿é—®ç«¯å£
- é…ç½®SSL/TLSåŠ å¯†
- è®¾ç½®è®¿é—®æ§åˆ¶åˆ—è¡¨(ACL)

## ğŸ“ æŠ€æœ¯æ”¯æŒ

å¦‚é‡åˆ°é—®é¢˜ï¼Œè¯·ï¼š

1. æŸ¥çœ‹æœ¬æ–‡æ¡£çš„æ•…éšœæ’é™¤éƒ¨åˆ†
2. æ£€æŸ¥æ—¥å¿—æ–‡ä»¶è·å–è¯¦ç»†é”™è¯¯ä¿¡æ¯
3. è¿è¡Œ `./deploy_commands.sh troubleshoot`
4. æäº¤Issueå¹¶é™„ä¸Šç¯å¢ƒä¿¡æ¯å’Œé”™è¯¯æ—¥å¿—

---

## ğŸ“„ æ–‡ä»¶è¯´æ˜

- `cluster_config.json` - é›†ç¾¤é…ç½®æ–‡ä»¶
- `launch_cluster.py` - é›†ç¾¤å¯åŠ¨è„šæœ¬
- `vllm_distributed_master.sh` - MasterèŠ‚ç‚¹å¯åŠ¨è„šæœ¬
- `vllm_distributed_worker.sh` - WorkerèŠ‚ç‚¹å¯åŠ¨è„šæœ¬
- `deploy_commands.sh` - éƒ¨ç½²å‘½ä»¤é›†åˆ
- `test_distributed.py` - åˆ†å¸ƒå¼æµ‹è¯•è„šæœ¬
- `Dockerfile` - Dockeré•œåƒæ„å»ºæ–‡ä»¶
- `k8s-deployment.yaml` - Kuberneteséƒ¨ç½²é…ç½®
- `requirements.txt` - Pythonä¾èµ–åˆ—è¡¨